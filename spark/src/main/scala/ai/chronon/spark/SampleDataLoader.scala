package ai.chronon.spark

import java.nio.file.{Files, Paths}

import ai.chronon.spark.SampleHelper.getPriorRunManifestMetadata
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.DataFrame

/*
This class works in conjunction with Sample.scala
Given a path to the output of the Sample job (which writes data to local disk), this picks up the files
and creates tables with them in a local data warehouse.
It maintains its own manifest file to track what tables need updating, to ensure that we spend as little time
as possible loading tables in between sampled data runs.
 */
object SampleDataLoader {

  def loadTable(directory: String, session: SparkSession): Unit = {
    val df: DataFrame = session.read.parquet(directory)
    val folderName = Paths.get(directory).getFileName.toString
    val splits = folderName.split("\\.")
    assert(splits.nonEmpty && splits.size == 2,
      "local data directory must be named `namespace.table`. This should be auto-generated by Sample.scala. Check" +
        "path or job logic.")
    session.sql(s"DROP TABLE IF EXISTS $folderName")
    df.write.partitionBy("ds").saveAsTable(folderName)
  }

  def loadAllData(sampleDirectory: String, session: SparkSession): Unit = {
    val basePath = Paths.get(sampleDirectory)
    val directoriesStream = Files.newDirectoryStream(basePath)

    val sampleManifest: Map[String, Int] = getPriorRunManifestMetadata(sampleDirectory)


  }

}
