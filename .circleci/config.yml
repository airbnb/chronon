version: 2.1

base_defaults: &base_defaults
    resource_class: xlarge
    working_directory: /chronon

executors:
    docker_baseimg_executor:
        resource_class: xlarge
        working_directory: /chronon
        docker:
            - image: houpy0829/chronon-ci:base--f87f50dc520f7a73894ae024eb78bd305d5b08e2
    docker_baseimg_executor_xxlarge:
      resource_class: 2xlarge
      working_directory: /chronon
      docker:
        - image: houpy0829/chronon-ci:base--f87f50dc520f7a73894ae024eb78bd305d5b08e2

jobs:
    "Pull Docker Image":
        <<: *base_defaults
        docker:
            - image: docker:17.05.0-ce-git
        steps:
            - setup_remote_docker:
                docker_layer_caching: true
            - checkout
            - run:
                name: Pull existing docker image
                command: |
                    set +o pipefail
                    docker pull houpy0829/chronon-ci:base--f87f50dc520f7a73894ae024eb78bd305d5b08e2 || true

    "Scala 12 -- Spark 3 Tests":
        executor: docker_baseimg_executor_xxlarge
        parallelism: 10
        steps:
            - checkout
            - restore_cache:
                keys:
                  - sbt-cache-v1-{{ checksum "build.sbt" }}-{{ checksum "project/plugins.sbt" }}
                  - sbt-cache-v1-{{ checksum "build.sbt" }}
                  - sbt-cache-v1-
            - run:
                  name: Run Spark 3.1.1 tests (parallelized)
                  shell: /bin/bash -leuxo pipefail
                  command: |
                      conda activate chronon_py
                      # Increase if we see OOM.
                      export SBT_OPTS="-XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=4G -Xmx4G -Xms2G"

                      # Split tests across parallel containers, excluding Fetcher tests
                      TESTFILES=$(find spark/src/test/scala aggregator/src/test/scala -name "*Test.scala" | \
                        grep -v "FetcherTest" | \
                        sort | \
                        circleci tests split --split-by=timings --timings-type=filename)

                      # Convert file paths to fully qualified class names and run tests
                      for filepath in $TESTFILES; do
                        if [[ $filepath == spark/* ]]; then
                          module="spark_uber"
                        elif [[ $filepath == aggregator/* ]]; then
                          module="aggregator"
                        else
                          continue
                        fi

                        classname=$(echo "$filepath" | \
                          sed 's|spark/src/test/scala/||; s|aggregator/src/test/scala/||' | \
                          sed 's|/|.|g' | \
                          sed 's|\.scala$||')
                        echo "Running test: $module / $classname"
                        sbt "++ 2.12.12; $module/testOnly $classname"
                      done
            - store_test_results:
                  path: /chronon/spark/target/test-reports
            - store_test_results:
                  path: /chronon/aggregator/target/test-reports
            - run:
                  name: Compress spark-warehouse
                  command: |
                    cd /tmp/ && tar -czvf spark-warehouse.tar.gz chronon/spark-warehouse
                  when: on_fail
            - store_artifacts:
                  path: /tmp/spark-warehouse.tar.gz
                  destination: spark_warehouse.tar.gz
                  when: on_fail
            - save_cache:
                key: sbt-cache-v1-{{ checksum "build.sbt" }}-{{ checksum "project/plugins.sbt" }}
                paths:
                  - ~/.cache/coursier
                  - ~/.ivy2/cache
                  - ~/.sbt

    "Scala 13 -- Tests":
        executor: docker_baseimg_executor
        parallelism: 8
        steps:
            - checkout
            - restore_cache:
                keys:
                  - sbt-cache-v1-{{ checksum "build.sbt" }}-{{ checksum "project/plugins.sbt" }}
                  - sbt-cache-v1-{{ checksum "build.sbt" }}
                  - sbt-cache-v1-
            - run:
                  name: Run Scala 13 tests (parallelized)
                  shell: /bin/bash -leuxo pipefail
                  command: |
                      conda activate chronon_py
                      # Increase if we see OOM.
                      export SBT_OPTS="-XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=8G -Xmx8G -Xms4G"

                      # Split tests across parallel containers, excluding Fetcher tests
                      TESTFILES=$(find spark/src/test/scala aggregator/src/test/scala -name "*Test.scala" | \
                        grep -v "FetcherTest" | \
                        sort | \
                        circleci tests split --split-by=timings --timings-type=filename)

                      # Convert file paths to fully qualified class names and run tests
                      for filepath in $TESTFILES; do
                        if [[ $filepath == spark/* ]]; then
                          module="spark_uber"
                        elif [[ $filepath == aggregator/* ]]; then
                          module="aggregator"
                        else
                          continue
                        fi

                        classname=$(echo "$filepath" | \
                          sed 's|spark/src/test/scala/||; s|aggregator/src/test/scala/||' | \
                          sed 's|/|.|g' | \
                          sed 's|\.scala$||')
                        echo "Running test: $module / $classname"
                        sbt "++ 2.13.6; $module/testOnly $classname"
                      done
            - store_test_results:
                  path: /chronon/spark/target/test-reports
            - store_test_results:
                  path: /chronon/aggregator/target/test-reports
            - run:
                  name: Compress spark-warehouse
                  command: |
                    cd /tmp/ && tar -czvf spark-warehouse.tar.gz chronon/spark-warehouse
                  when: on_fail
            - store_artifacts:
                  path: /tmp/spark-warehouse.tar.gz
                  destination: spark_warehouse.tar.gz
                  when: on_fail
            - save_cache:
                key: sbt-cache-v1-{{ checksum "build.sbt" }}-{{ checksum "project/plugins.sbt" }}
                paths:
                  - ~/.cache/coursier
                  - ~/.ivy2/cache
                  - ~/.sbt

    "Scala 11 -- Compile":
      executor: docker_baseimg_executor
      steps:
        - checkout
        - restore_cache:
            keys:
              - sbt-cache-v1-{{ checksum "build.sbt" }}-{{ checksum "project/plugins.sbt" }}
              - sbt-cache-v1-{{ checksum "build.sbt" }}
              - sbt-cache-v1-
        - run:
            name: Compile Scala 11
            shell: /bin/bash -leuxo pipefail
            command: |
              conda activate chronon_py
              # Increase if we see OOM.
              export SBT_OPTS="-XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=4G -Xmx4G -Xms2G"
              sbt "++ 2.11.12 compile"
        - save_cache:
            key: sbt-cache-v1-{{ checksum "build.sbt" }}-{{ checksum "project/plugins.sbt" }}
            paths:
              - ~/.cache/coursier
              - ~/.ivy2/cache
              - ~/.sbt

    "Fetcher Tests":
        executor: docker_baseimg_executor
        steps:
            - checkout
            - restore_cache:
                keys:
                  - sbt-cache-v1-{{ checksum "build.sbt" }}-{{ checksum "project/plugins.sbt" }}
                  - sbt-cache-v1-{{ checksum "build.sbt" }}
                  - sbt-cache-v1-
            - run:
                  name: Run Fetcher tests
                  shell: /bin/bash -leuxo pipefail
                  command: |
                      conda activate chronon_py
                      export SBT_OPTS="-XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=4G -Xmx4G -Xms2G"
                      sbt "++ 2.12.12; spark_uber/testOnly ai.chronon.spark.test.FetcherTest ai.chronon.spark.test.ChainingFetcherTest ai.chronon.spark.test.JavaFetcherTest"
            - store_test_results:
                  path: /chronon/spark/target/test-reports
            - save_cache:
                key: sbt-cache-v1-{{ checksum "build.sbt" }}-{{ checksum "project/plugins.sbt" }}
                paths:
                  - ~/.cache/coursier
                  - ~/.ivy2/cache
                  - ~/.sbt

    "Chronon Python Lint":
        executor: docker_baseimg_executor
        steps:
            - checkout
            - run:
                  name: Run Chronon Python lint
                  shell: /bin/bash -leuxo pipefail
                  command: |
                      conda activate chronon_py
                      cd /chronon/api/py/ai/chronon
                      pip install importlib-metadata==4.11.4 #Install importlib-metadata < 5
                      flake8 --extend-ignore=W605,Q000,F631,E203

    "Chronon Python Tests":
        executor: docker_baseimg_executor
        steps:
            - checkout
            - restore_cache:
                keys:
                  - sbt-cache-v1-{{ checksum "build.sbt" }}-{{ checksum "project/plugins.sbt" }}
                  - sbt-cache-v1-{{ checksum "build.sbt" }}
                  - sbt-cache-v1-
            - run:
                  name: Build Chronon JARs for PySpark tests
                  shell: /bin/bash -leuxo pipefail
                  command: |
                    conda activate chronon_py
                    export SBT_OPTS="-XX:+CMSClassUnloadingEnabled -Xmx4G -Xms2G"
                    sbt "++ 2.12.12; publishLocal"
            - run:
                  name: Run Chronon Python tests
                  shell: /bin/bash -leuxo pipefail
                  command: |
                    conda activate chronon_py
                    pushd /chronon/api/
                    thrift --gen py -out /chronon/api/py/ai/chronon\
                       /chronon/api/thrift/api.thrift                 # Generate thrift files
                    cd /chronon/                                      # Go to Python module
                    pip install -r api/py/requirements/dev.txt        # Install latest requirements
                    tox                                               # Run tests
                    popd
            - store_artifacts:
                  path: /chronon/api/py/htmlcov
            - save_cache:
                key: sbt-cache-v1-{{ checksum "build.sbt" }}-{{ checksum "project/plugins.sbt" }}
                paths:
                  - ~/.cache/coursier
                  - ~/.ivy2/cache
                  - ~/.sbt

    "Scalafmt Check":
        executor: docker_baseimg_executor
        steps:
            - checkout
            - restore_cache:
                keys:
                  - sbt-cache-v1-{{ checksum "build.sbt" }}-{{ checksum "project/plugins.sbt" }}
                  - sbt-cache-v1-{{ checksum "build.sbt" }}
                  - sbt-cache-v1-
            - run:
                  name: Run ScalafmtCheck
                  shell: /bin/bash -leuxo pipefail
                  command: |
                      conda activate chronon_py
                      sbt +scalafmtCheck
            - save_cache:
                key: sbt-cache-v1-{{ checksum "build.sbt" }}-{{ checksum "project/plugins.sbt" }}
                paths:
                  - ~/.cache/coursier
                  - ~/.ivy2/cache
                  - ~/.sbt
    # run these separately as we need a isolated JVM to not have Spark session settings interfere with other runs
    # long term goal is to refactor the current testing spark session builder and avoid adding new single test to CI
    "Scala 13 -- Iceberg Format Tests":
      executor: docker_baseimg_executor
      steps:
        - checkout
        - restore_cache:
            keys:
              - sbt-cache-v1-{{ checksum "build.sbt" }}-{{ checksum "project/plugins.sbt" }}
              - sbt-cache-v1-{{ checksum "build.sbt" }}
              - sbt-cache-v1-
        - run:
            name: Run Scala 13 tests for Iceberg format
            environment:
              format_test: iceberg
            shell: /bin/bash -leuxo pipefail
            command: |
              conda activate chronon_py
              # Increase if we see OOM.
              export SBT_OPTS="-XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=4G -Xmx4G -Xms2G"
              sbt ';project spark_embedded; ++ 2.13.6; testOnly ai.chronon.spark.test.TableUtilsFormatTest'
        - store_test_results:
            path: /chronon/spark/target/test-reports
        - store_test_results:
            path: /chronon/aggregator/target/test-reports
        - run:
            name: Compress spark-warehouse
            command: |
              cd /tmp/ && tar -czvf spark-warehouse.tar.gz chronon/spark-warehouse
            when: on_fail
        - store_artifacts:
            path: /tmp/spark-warehouse.tar.gz
            destination: spark_warehouse.tar.gz
            when: on_fail
        - save_cache:
            key: sbt-cache-v1-{{ checksum "build.sbt" }}-{{ checksum "project/plugins.sbt" }}
            paths:
              - ~/.cache/coursier
              - ~/.ivy2/cache
              - ~/.sbt
    "Scala 13 -- Iceberg Table Utils Tests":
      executor: docker_baseimg_executor
      steps:
        - checkout
        - restore_cache:
            keys:
              - sbt-cache-v1-{{ checksum "build.sbt" }}-{{ checksum "project/plugins.sbt" }}
              - sbt-cache-v1-{{ checksum "build.sbt" }}
              - sbt-cache-v1-
        - run:
            name: Run Scala 13 tests for Iceberg Table Utils
            environment:
              format_test: iceberg
            shell: /bin/bash -leuxo pipefail
            command: |
              conda activate chronon_py
              # Increase if we see OOM.
              export SBT_OPTS="-XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=4G -Xmx4G -Xms2G"
              sbt ';project spark_embedded; ++ 2.13.6; testOnly ai.chronon.spark.test.TableUtilsTest'
        - store_test_results:
            path: /chronon/spark/target/test-reports
        - store_test_results:
            path: /chronon/aggregator/target/test-reports
        - run:
            name: Compress spark-warehouse
            command: |
              cd /tmp/ && tar -czvf spark-warehouse.tar.gz chronon/spark-warehouse
            when: on_fail
        - store_artifacts:
            path: /tmp/spark-warehouse.tar.gz
            destination: spark_warehouse.tar.gz
            when: on_fail
        - save_cache:
            key: sbt-cache-v1-{{ checksum "build.sbt" }}-{{ checksum "project/plugins.sbt" }}
            paths:
              - ~/.cache/coursier
              - ~/.ivy2/cache
              - ~/.sbt

workflows:
    build_test_deploy:
        jobs:
            - "Pull Docker Image"
            - "Scala 11 -- Compile":
                requires:
                  - "Pull Docker Image"
            - "Fetcher Tests":
                  requires:
                      - "Pull Docker Image"
            - "Scala 12 -- Spark 3 Tests":
                  requires:
                      - "Pull Docker Image"
            - "Scala 13 -- Tests":
                  requires:
                      - "Pull Docker Image"
            - "Scalafmt Check":
                  requires:
                      - "Pull Docker Image"
            - "Chronon Python Tests":
                  requires:
                      - "Pull Docker Image"
            - "Chronon Python Lint":
                  requires:
                      - "Pull Docker Image"
            - "Scala 13 -- Iceberg Format Tests":
                  requires:
                      - "Pull Docker Image"
            - "Scala 13 -- Iceberg Table Utils Tests":
                  requires:
                      - "Pull Docker Image"