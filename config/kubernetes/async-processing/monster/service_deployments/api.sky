# DO NOT EDIT: http://go/vendor-skycfg
load("config/kubernetes/apps/deployment.sky", "deployment", "deployment_options")
load("config/kubernetes/core/container.sky", "container_port")
load("config/kubernetes/core/env_var.sky", "container_env_vars")
load("config/kubernetes/helpers/healthcheck.sky", "healthchecked_service")
load("config/kubernetes/helpers/images.sky", "image")
load("config/kubernetes/helpers/security.sky", "add_security_groups")
load("config/kubernetes/networking/public/config.sky", "networking_config", "global_ratelimit")
load("config/kubernetes/sidecars/config_srv.sky", "config_srv_sidecar")
load("config/kubernetes/sidecars/consul.sky", "consul_service", "consul_grpc_check")
load("config/kubernetes/stripe.sky", "stripe_pod")


load("config/kubernetes/async-processing/monster/config.sky", "get_msp_availability_tier")
load(
    "config/kubernetes/async-processing/monster/util.sky",
    "format_isolation_group",
    "generate_consul_name",
    "get_jmx_java_options",
    "get_monster_env_cmd_arguments",
    "get_worker_envoy_address_from_consul",
    "monster_deployment_labels",
)
load("config/kubernetes/async-processing/monster/jmxfetch.sky", "add_jmxfetch_sidecar")


def monster_api_deployment(ctx, config):
    """Generates a Monster API deployment for a specific isolation group

    Args:
        ctx: The skycfg context variable, see http://go/disky
        config: shared metadata data structure provided by config.sky

    Returns:
        A deployment representing a Monster API service with the passed-in
        configuration applied

    """
    ig = format_isolation_group(config.monster_isolation_group)

    consul_service_name = generate_consul_name(ig, config.monster_service)
    host_type = "monsterapi"
    mongo_address = "127.0.0.1:10087"
    # To see what ports are already used up, inspect a pod-level page from
    # https://amp.corp.stripe.com/services/monster-api
    port = 17000
    health_port = 17001
    jmx_port = 8086

    # This should be the connection pool size (defaults to 100), as we expect this to be our bottleneck factor on monster-api concurrency
    max_concurrent_requests = 100
    mongo_connection_pool_max_size = 5

    runtime_args = [
        ["./src/scala/com/stripe/monster/api/monster-api"],
        ["--port", str(port)],
        ["--health-port", str(health_port)],
        ["--max-concurrent-requests", str(max_concurrent_requests)],
        ["--mongo-connection-pool-max-size", str(mongo_connection_pool_max_size)],
        ["--mongo-address", mongo_address],
        ["--config-addr", get_worker_envoy_address_from_consul(generate_consul_name(ig, "config-workers"))],
        get_monster_env_cmd_arguments(ctx, config.monster_isolation_group, host_type)
    ]

    deploy = deployment(
        ctx,
        stripe_pod(
            ctx,
            name = consul_service_name,
            instance_type = config.aws_instance_size,
            availability_tier = get_msp_availability_tier(config.monster_availability_tier),
            namespace = host_type,
            container_name = "monster-api", # unified sourcetype field for Splunk
            image = image(ctx, artifact = "monster-api-image"),
            command = [el for arg in runtime_args for el in arg],
        ),
        container_env_vars(
            vars = {
                "JAVA_OPTS": " ".join([
                    "-Xmx{}m".format(config.raw_memory_mb),
                    "-XX:+ExitOnOutOfMemoryError",
                    "-XX:+UseG1GC",
                    "-XX:MaxGCPauseMillis=50",
                ] + get_jmx_java_options(jmx_port)),
            },
        ),

        # identify the deployment
        monster_deployment_labels(config),
        container_port(port),

        # Add jmx metrics sidecar
        container_port(jmx_port),
        add_jmxfetch_sidecar(ctx, "monster-api", jmx_port),
        # Generic healthcheck for deployment control

        healthchecked_service(
            ctx,
            port = health_port,
            name = "monster-api",
            # When we're instructed to shutdown by a SIGTERM, this is how long we'll wait (in seconds)
            # to gracefully drain traffic before we're forcibly stopped by a SIGKILL
            #
            # monster-api only needs to finish inserting FQEs into Mongo or publishing to Kafka
            # https://git.corp.stripe.com/stripe-internal/zoolander/blob/8869919e8133bfb61f3963b2d55709edbf14aa43/src/scala/com/stripe/monster/utils/DBUtils.scala#L19-L26

            #
            # Context:
            # https://paper.dropbox.com/doc/Investigation-Traffic-Draining-for-Monster-on-Shared-MSP-byFrxy0nX26Vz51t28ois
            # https://confluence.corp.stripe.com/display/CDS/Safe+Draining+on+Shared+MSP
            request_drain_grace_period_seconds = 60,
        ),

        # per-host-set Consul registry for SRV-based work sharing
        consul_service(
            port = port,
            name = consul_service_name,
            meta = {"lb_policy": "LEAST_REQUEST"},
            tags = ["grpc"],
            checks = [consul_grpc_check(port = port)],
        ),
        config_srv_sidecar(ctx),
        networking_config(
            mproxy_tier = "monster",
            global_ratelimit = global_ratelimit(enable = True),
        ),
        # We limit how many previous revisions of ReplicaSet's we keep around to limit
        # the amount of metadata the k8s control plane has to serve up for Monster
        # See: https://jira.corp.stripe.com/browse/RUN_ORCH-11446, http://go/ir/gray-fuse, http://go/ir/whim-tangent
        # And: https://unofficial-kubernetes.readthedocs.io/en/latest/concepts/workloads/controllers/deployment/#revision-history-limit
        deployment_options(revisionHistoryLimit = 3),
        replicas = config.replicas,
        strategy = config.strategy,
        shared_msp = True,
    )
    return deploy
