# DO NOT EDIT: http://go/vendor-skycfg
load("config/kubernetes/apps/deployment.sky", "deployment", "deployment_options")
load("config/kubernetes/async-processing/monster/config.sky", "get_msp_availability_tier")
load("config/kubernetes/async-processing/monster/jmxfetch.sky", "add_jmxfetch_sidecar")
load("config/kubernetes/async-processing/monster/util.sky", "format_isolation_group", "generate_consul_name", "get_jmx_java_options", "get_monster_env_cmd_arguments", "get_worker_envoy_address_from_consul", "monster_deployment_labels")
load("config/kubernetes/core/container.sky", "container_port")
load("config/kubernetes/core/env_var.sky", "container_env_vars")
load("config/kubernetes/helpers/context.sky", "get_cluster", "get_env")
load("config/kubernetes/helpers/healthcheck.sky", "healthchecked_service")
load("config/kubernetes/helpers/images.sky", "image")
load("config/kubernetes/helpers/security.sky", "add_security_groups")
load("config/kubernetes/networking/public/config.sky", "networking_config", "global_ratelimit")
load("config/kubernetes/sidecars/config_srv.sky", "config_srv_sidecar")
load("config/kubernetes/sidecars/consul.sky", "consul_service", "consul_http_check")
load("config/kubernetes/stripe.sky", "stripe_pod")

def is_fanout_retry(config):
    return config.monster_service == "fanout-retry"


# NOTE: This name becomes the kafka consumer group name. It uses the unformatted version of the isolation group
def generate_name(config):
    if is_fanout_retry(config):
        return "monster-queuing-only-{}-retry".format(config.monster_isolation_group)
    else:
        return "monster-queuing-only-{}".format(config.monster_isolation_group)


def generate_derive_input_topics_for_host_set(config):
    if is_fanout_retry(config):
        return []

    return ["--derive-input-topics-for-host-set", config.monster_isolation_group]


def generate_input(config):
    if is_fanout_retry(config):
        return ["--input", "http-driver.monster-queuing.{}.retries:events".format(config.monster_isolation_group)]

    # --input provides additional topics that monster-fanout should read from to get events,
    # besides the ones derived from the host-set
    result = [
        "--input",
        "monster.insert.retries.{}:submission".format(config.monster_isolation_group)
    ]

    if config.monster_isolation_group == "general":
        result += ["--input", "monster.insert.ffqe-requeue:submission"]

    return result


def generate_output(config):
    # Fanout's regular "output" is FQEs into Mongo; these output topics are the unhappy paths:
    # retries and deadlettering (at queuing or insertion time)
    return [
        "--output",
        "http-driver.monster-queuing.{}.retries:events".format(config.monster_isolation_group),
        "--output",
        "monster.queuing-deadletter.{}:deadletters".format(config.monster_isolation_group),
    ]


def generate_submission_dead_letter_topic(config):
    return [
        "--submission-dead-letter-topic",
        "monster.insert.deadletter.{}".format(config.monster_isolation_group)
    ]


def generate_destination(config):
    return [
        "--destination",
        "http://{monster_fanout_workers}/v4/monster/queue/{isolation_group}".format(
            monster_fanout_workers = get_worker_envoy_address_from_consul(generate_consul_name(config.monster_isolation_group, "fanout-workers")),
            isolation_group = config.monster_isolation_group
        )
    ]


def generate_monster_config_service(config):
    return [
        "--monster-config-service",
        get_worker_envoy_address_from_consul(
            generate_consul_name(config.monster_isolation_group, "config-workers")
        ),
    ]


def generate_parallel(config):
    parallel = 64 if is_fanout_retry(config) else 256

    return ["--parallel", str(parallel)]


def generate_request_timeout(config):
    # RUN_STREAMING-41303
    # ===================
    #
    # Default is 15s. Increasing this value to deal with a poison message
    # looping around retry queues. Although it seems like the queuing endpoint
    # does end up returning a result, the Fanout client stops listening, and
    # if that happens, the message is just shunted back into the retry queue
    # endlessly.
    #
    # Data from Splunk shows that the max time to return over the past two
    # days has been 75 s, so I'm picking 90 s.
    #
    # Revert this change once the queues are purged.
    return ["--request-timeout", str(90)] # in seconds


def generate_total_timeout(config):
    # We almost always prefer to retry in-process on errors from the Ruby workers
    # rather than crash (which is what we'll do when this expires). Crashes force a
    # Kafka rebalance plus restart any exponential backoffs.
    #
    # The only reason to keep this timeout finite is as a hedge against hanging
    # indefinitely in some unexpected pathological scenario; we know we'll at least
    # see crashes in logs.
    return ["--total-timeout", str(600)] # in seconds


def generate_batch(config):
    if is_fanout_retry(config):
        batch_size = 1
    elif format_isolation_group(config.monster_isolation_group) in ["general", "model-ingestion", "monster-test"]:
        batch_size = 20
    else:
        batch_size = 10

    return ["--batch", str(batch_size)]


def generate_max_batches_per_partition(config):
    ig = format_isolation_group(config.monster_isolation_group)
    if is_fanout_retry(config):
        max_batches_per_partition = 5
    elif ig == "general":
        # Kproxy coalesces partitions by topic, but with >16 hosts, most topics will
        # have at most one partition per box.
        #
        # However, reporting.events gets around 8k messages/sec, meaning that each
        # partition is 500 messages/sec. With a batch size of 10, we need to do 50
        # batches/sec to not fall behind, much less catch up. The P99 batch latency
        # is around 300ms, so that doesn't leave a lot of headroom. Increasing it
        # to 50.
        # TODO(haran) update me? general has batch size of 20
        max_batches_per_partition = 50
    elif ig == "sigma":
        # Total in-flight requests is controlled by parallelism BUT also by
        # max_batches_per_partition. Kproxy treats all of
        # aqueduct.hydrator_batch_messages as a single partition.
        #
        # Total parallelism = parallelism * number of fanout processes.
        #
        # We want total in flight requests to match total parallelism (parallelism controls
        # the size of the http connection pool per fanout process).
        #
        # parallelism * num fanout hosts = total parallelism = 256 * 3 = 768
        max_batches_per_partition = 768
    else:
        # Kproxy coalesces partitions by topic. For 4 monster-fanout hosts, a
        # single node may have 1-4 partitions for each topic. If we choose 80, we
        # get the equivalent in throughput of 20 batches per partition.
        max_batches_per_partition = 80

    return ["--max-batches-per-partition", str(max_batches_per_partition)]


def generate_max_total_messages_pending(config):
    ig = format_isolation_group(config.monster_isolation_group)
    if is_fanout_retry(config):
        max_total_messages_pending = 1000
    elif ig == "general":
        max_total_messages_pending = 3000
    elif ig == "sigma":
        max_total_messages_pending = 1000
    else:
        max_total_messages_pending = 2147483647 # Int.MaxValue

    return ["--max-total-messages-pending", str(max_total_messages_pending)]


def generate_delay_consumer_ms(config):
    if is_fanout_retry(config):
        return ["--delay-consumer-ms", str(5*60*1000)]

    return []


def generate_kproxy_service(config):
    # This sets our priority tier when talking to kproxy

    if format_isolation_group(config.monster_isolation_group) in ["general", "model-ingestion", "sigma"]:
        kproxy_service = "kproxy-consumer-a200"
    else:
        kproxy_service = "kproxy-consumer"

    return ["--kproxy-service", kproxy_service]


def generate_fqe_mongo_connection_pool_max_size(ctx, config):
    if format_isolation_group(config.monster_isolation_group) in ["issuing"]:
        # in ir-authentic-inaugurate, issuing host set exceeded the Envoy connection circuit breaker limit
        # we want to use a smaller connection pool size to avoid recurrence during traffic spikes
        fqe_mongo_connection_pool_max_size = 5
    elif not is_fanout_retry(config) and get_cluster(ctx) == "northwest":
        fqe_mongo_connection_pool_max_size = 10
    else:
        fqe_mongo_connection_pool_max_size = 5

    return ["--fqe-mongo-connection-pool-max-size", str(fqe_mongo_connection_pool_max_size)]

def generate_max_response_bytes(config):
    if config.monster_isolation_group == "general":
        max_response_bytes = 40 * 1024 * 1024
    else:
        max_response_bytes = 20 * 1024 * 1024

    return ["--max-response-bytes", str(max_response_bytes)]

def monster_fanout_deployment(ctx, config):
    """Generates a Fanout deployment for a specific isolation group

    Args:
        ctx: The skycfg context variable, see http://go/disky
        config: shared metadata data structure provided by config.sky

    Returns:
        A deployment representing a Fanout service with the passed-in
        configuration applied
    """

    name = generate_name(config)
    mongo_address = "127.0.0.1:10087"
    host_type = "monsterfanout"
    port = 8080

    # list of lists, empty lists are filtered when creating stripe pod with list comprehension
    runtime_args = [
        ["src/scala/com/stripe/monster/fanout/{service_name}_binary".format(service_name=config.monster_service)],
        generate_derive_input_topics_for_host_set(config),
        generate_input(config),
        generate_output(config),
        ["--submission-dead-letter-topic", "monster.insert.deadletter.{}".format(config.monster_isolation_group)],
        generate_destination(config),
        generate_monster_config_service(config),
        ["--name", name],
        generate_parallel(config),
        generate_request_timeout(config),
        generate_total_timeout(config),
        generate_batch(config),
        generate_max_batches_per_partition(config),
        generate_max_total_messages_pending(config),
        generate_delay_consumer_ms(config),
        generate_kproxy_service(config),
        ["--poll-timeout-ms", str(500)],
        generate_max_response_bytes(config),
        ["--healthcheck-port", str(port)],
        ["--fqe-mongo-app-name", name],
        ["--fqe-mongo-address", mongo_address],
        generate_fqe_mongo_connection_pool_max_size(ctx, config),
        ["--ffqe-mongo-app-name", name],
        ["--ffqe-mongo-address", mongo_address],
        ["--ffqe-mongo-connection-pool-max-size", str(5)],
        ["--mongo-app-name", name],
        ["--mongo-address", mongo_address],
        ["--mongo-connection-pool-max-size", str(100)],
        ["--mongo-connection-pool-max-wait-queue-size", str(10000)],
        get_monster_env_cmd_arguments(ctx, config.monster_isolation_group, host_type)
    ]

    jmx_port = 8086
    consul_service_name = generate_consul_name(config.monster_isolation_group, config.monster_service)

    deploy = deployment(
        ctx,
        stripe_pod(
            ctx,
            name = consul_service_name,
            instance_type = config.aws_instance_size,
            availability_tier = get_msp_availability_tier(config.monster_availability_tier),
            namespace = host_type,
            container_name = "monster-{}".format(config.monster_service), # unified sourcetype field for Splunk
            image = image(ctx, artifact = "monster-{}-image".format(config.monster_service)),
            command = [el for arg in runtime_args for el in arg],
            requires_consul = False,
        ),
        container_env_vars(
            vars = {
                "JAVA_OPTS": " ".join([
                    "-Xmx{}m".format(config.raw_memory_mb),
                    "-XX:+ExitOnOutOfMemoryError",
                    "-Djava.io.tmpdir=/pay/tmp",
                    "-Djava.util.logging.config.file=/src/scala/com/stripe/monster/fanout/logging.properties",
                ] + get_jmx_java_options(jmx_port))
            },
        ),

        container_port(jmx_port),
        add_jmxfetch_sidecar(ctx, "monster-{}".format(config.monster_service), jmx_port),

        # identify the deployment
        monster_deployment_labels(config),

        # generic healthcheck for deployment control
        healthchecked_service(
            ctx,
            port = port,
            name = "monster-{}".format(config.monster_service),
            # When we're instructed to shutdown by a SIGTERM, this is how long we'll wait (in seconds)
            # to gracefully drain traffic before we're forcibly stopped by a SIGKILL
            #
            # monster-fanout only needs to finish talking to Mongo to insert FQEs:
            # https://git.corp.stripe.com/stripe-internal/zoolander/blob/8869919e8133bfb61f3963b2d55709edbf14aa43/src/scala/com/stripe/monster/utils/DBUtils.scala#L19-L26
            #
            # Read more:
            # https://paper.dropbox.com/doc/Investigation-Traffic-Draining-for-Monster-on-Shared-MSP-byFrxy0nX26Vz51t28ois
            # https://confluence.corp.stripe.com/display/CDS/Safe+Draining+on+Shared+MSP https://confluence.corp.stripe.com/display/CDS/Safe+Draining+on+Shared+MSP
            request_drain_grace_period_seconds = 60
        ),

        # per-host-set Consul registry for SRV-based work sharing
        consul_service(
            port = port,
            name = consul_service_name,
            checks = [consul_http_check(path = "/healthcheck", port = port)],
        ),

        add_security_groups(
            "kafkahttpconsumer",
            "monsterfanout",
        ),
        config_srv_sidecar(ctx),
        networking_config(
            mproxy_tier = "monster",
            global_ratelimit = global_ratelimit(enable = True),
        ),
        # We limit how many previous revisions of ReplicaSet's we keep around to limit
        # the amount of metadata the k8s control plane has to serve up for Monster
        # See: https://jira.corp.stripe.com/browse/RUN_ORCH-11446, http://go/ir/gray-fuse, http://go/ir/whim-tangent
        # And: https://unofficial-kubernetes.readthedocs.io/en/latest/concepts/workloads/controllers/deployment/#revision-history-limit
        deployment_options(revisionHistoryLimit = 3),
        replicas = config.replicas,
        strategy = config.strategy,
        shared_msp = True,
    )
    return deploy
