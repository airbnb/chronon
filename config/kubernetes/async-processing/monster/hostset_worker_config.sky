# DO NOT EDIT: http://go/vendor-skycfg
# This file defines replica sets that correspond to Monster host sets

load("config/kubernetes/async-processing/monster/config.sky",
     "AVAILABILITY_TIER_A100", "AVAILABILITY_TIER_A200", "AVAILABILITY_TIER_A400", "AVAILABILITY_TIER_A400_AUTO")
load("config/kubernetes/async-processing/monster/hostset_worker.sky", "cluster", "hostset")
load(
    "config/kubernetes/helpers/aws_instance_sizes.sky",
    "m6g_16xlarge", "r6g_8xlarge", "r6g_xlarge", "r7g_xlarge", "r7g_8xlarge", "r6g_4xlarge"
)
load("config/kubernetes/crds/keda/scaledobject.sky", "scaled_object", "prometheus_trigger", "cpu_trigger", "PROMETHEUS_METRIC_TYPE_VALUE", "scale_policy", "memory_trigger", "schedule_trigger")

WORKER_AVAILABILITY_PROMQL = '''
1 -
sum by (service) (
  label_replace(
    sys_workers_available{{host_cluster="{cluster}", namespace="monsterworkersbox"}}
    ,"pod","$1","pod_id","(.*)"
  )
  *
  on(pod)
  group_left(label_monster_stripe_io_ig)
  kube_pod_labels{{host_cluster="{cluster}",label_monster_stripe_io_ig="{host_set}",label_monster_stripe_io_service="{service}", namespace="monsterworkersbox"}}
)
/
sum by (service) (
  label_replace(
    sys_workers_total{{host_cluster="{cluster}",namespace="monsterworkersbox"}}
    ,"pod","$1","pod_id","(.*)"
  )
  *
  on(pod)
  group_left(label_monster_stripe_io_ig)
  kube_pod_labels{{host_cluster="{cluster}",label_monster_stripe_io_ig="{host_set}", label_monster_stripe_io_service="{service}", namespace="monsterworkersbox"}}
)'''

def autoscaling_config(
    host_set,
    service,
    cluster,
    availability_threshold,
    cpu_threshold,
    min_replicas,
    max_replicas,
    scale_up_policy = None,
    scale_up_stabilization_window_seconds = None,
    scale_down_policy = None,
    scale_down_stabilization_window_seconds = None,
    enable_immediate_scaling = False,
    memory_threshold = None,
):

    prom = prometheus_trigger(
        query = WORKER_AVAILABILITY_PROMQL.format(cluster = cluster, host_set = host_set, service = service),
        threshold = availability_threshold,
        metric_type = PROMETHEUS_METRIC_TYPE_VALUE,
    )

    cpu = cpu_trigger(value = cpu_threshold)

    memory = None
    if memory_threshold != None:
        memory = memory_trigger(value = memory_threshold)

    return scaled_object(
        min_replicas = min_replicas,
        max_replicas = max_replicas,
        cpu_trigger = cpu,
        memory_trigger = memory,
        prometheus_triggers = [prom],
        scale_up_scaling_policies = [scale_up_policy] if scale_up_policy != None else None,
        scale_up_stabilization_window_seconds = scale_up_stabilization_window_seconds if scale_up_stabilization_window_seconds != None else None,
        scale_down_scaling_policies = [scale_down_policy] if scale_down_policy != None else None,
        scale_down_stabilization_window_seconds = scale_down_stabilization_window_seconds if scale_down_stabilization_window_seconds != None else None,
        enable_immediate_scaling = enable_immediate_scaling,
    )

def empty_worker_cluster():
  return cluster(
      consume_replicas=0,
      fanout_replicas=0)

def all_regions_empty_worker_cluster():
  return {
        "cmh":          empty_worker_cluster(),
        "northwest":    empty_worker_cluster(),
        "bom":          empty_worker_cluster(),
    }

A100_HOST_SETS = struct(
    availability_tier=AVAILABILITY_TIER_A100,
    hostsets=[
        hostset("high_priority_webhooks",
                is_canary=True,
                disable_preloading=False, # This host set has latency sensitive consumers and is not a good candidate for this feature.
                suffix="low-cpu",
                default=            cluster(mb_per_worker=2500),
                qa={
                    "northwest":    cluster(instance_type=r7g_xlarge),
                    "bom":          cluster(),
                    "cmh":          cluster(instance_type=r7g_xlarge),
                },
                preprod={
                    "northwest":    empty_worker_cluster(),
                    "bom":          empty_worker_cluster(),
                    "cmh":          cluster(instance_type=r7g_xlarge),
                },
                prod={
                    "northwest":    cluster(consume_replicas=4,
                                            fanout_replicas=3,
                                            instance_type=r7g_8xlarge,
                                            mb_per_worker=1000),
                    "bom":          cluster(consume_replicas=2, fanout_replicas=2, mb_per_worker=850),
                    "cmh":          cluster(consume_replicas=4,
                                            fanout_replicas=3,
                                            instance_type=r7g_8xlarge,
                                            mb_per_worker=1000),
                }),
        hostset("high_priority_webhooks",
                is_canary=False,
                disable_preloading=False, # This host set has latency sensitive consumers and is not a good candidate for this feature.
                suffix="low-cpu",
                default=            cluster(mb_per_worker=2500),
                qa={
                    "northwest":    cluster(instance_type=r7g_xlarge),
                    "bom":          cluster(instance_type=r6g_xlarge),
                    "cmh":          cluster(instance_type=r7g_xlarge),
                },
                prod={
                    "northwest":    cluster(consume_replicas=14,
                                            fanout_replicas=10,
                                            instance_type=r7g_8xlarge,
                                            mb_per_worker=1000),
                    "bom":          cluster(consume_replicas=3,
                                            fanout_replicas=4,
                                            mb_per_worker=850),
                    "cmh":          cluster(consume_replicas=14,
                                            fanout_replicas=10,
                                            instance_type=r7g_8xlarge,
                                            mb_per_worker=1000),
                }),
        hostset("checkout",
                is_canary=True,
                suffix="low-cpu",
                disable_preloading=True,
                default=            cluster(mb_per_worker=2500),
                qa={
                    "northwest":    cluster(),
                    "bom":          cluster(),
                    "cmh":          cluster(),
                },
                preprod={
                    "northwest":    empty_worker_cluster(),
                    "bom":          empty_worker_cluster(),
                    "cmh":          empty_worker_cluster(),
                },
                prod={
                    "bom":          cluster(consume_replicas=2),
                    "northwest":    cluster(consume_replicas=8,
                                            fanout_replicas=3),
                    "cmh":          cluster(consume_replicas=8,
                                            fanout_replicas=3),
                }),
        hostset("checkout",
                is_canary=False,
                suffix="low-cpu",
                disable_preloading=True,
                default=            cluster(mb_per_worker=2500),
                qa={
                    "northwest":    cluster(),
                    "bom":          cluster(),
                    "cmh":          cluster(),
                },
                preprod={
                    "northwest":    empty_worker_cluster(),
                    "bom":          empty_worker_cluster(),
                    "cmh":          empty_worker_cluster(),
                },
                prod={
                    "bom":          cluster(consume_replicas=10),
                    "northwest":    cluster(consume_replicas=10,
                                            fanout_replicas=5),
                    "cmh":          cluster(consume_replicas=10,
                                            fanout_replicas=5),
                }),
    ]
)

A200_HOST_SETS = struct(
    availability_tier=AVAILABILITY_TIER_A200,
    hostsets=[
        hostset("api-platform",
                is_canary=False,
                disable_preloading=True, # If re-enabling preloading, make sure to bump mb_per_worker to 2500 and scale accordingly
                suffix="low-cpu",
                qa={
                    "northwest":    cluster(mb_per_worker=1250, instance_type=r7g_xlarge),
                    "bom":          cluster(mb_per_worker=1250),
                    "cmh":          cluster(mb_per_worker=1250, instance_type=r7g_xlarge),
                },
                prod={
                    "northwest":    cluster(consume_replicas=21,
                                            fanout_replicas=6,
                                            mb_per_worker=900, # 263 workers
                                            instance_type=r7g_8xlarge),
                    "bom":          cluster(consume_replicas=12,
                                            fanout_replicas=3,
                                            mb_per_worker=1100),
                    "cmh":          cluster(consume_replicas=15,
                                            fanout_replicas=4,
                                            mb_per_worker=900, # 263 workers
                                            instance_type=r7g_8xlarge),
                }
        ),
        hostset("api-platform",
                is_canary=True,
                disable_preloading=True, # If re-enabling preloading, make sure to bump mb_per_worker to 2500 and scale accordingly
                suffix="low-cpu",
                qa={
                    "northwest":    cluster(mb_per_worker=1250, instance_type=r7g_xlarge),
                    "bom":          cluster(mb_per_worker=1250),
                    "cmh":          cluster(mb_per_worker=1250, instance_type=r7g_xlarge),
                },
                prod={
                    "northwest":    cluster(consume_replicas=4,
                                            mb_per_worker=900, # 263 workers
                                            instance_type=r7g_8xlarge),
                    "bom":          empty_worker_cluster(),
                    "cmh":          cluster(consume_replicas=4,
                                            mb_per_worker=900, # 263 workers
                                            instance_type=r7g_8xlarge),
                }
        ),
        hostset("billing",
                suffix="low-cpu",
                default=            cluster(mb_per_worker=2500),
                qa={
                    "northwest":    cluster(),
                    "bom":          cluster(),
                    "cmh":          cluster(),
                },
                preprod={
                    "northwest":    empty_worker_cluster(),
                    "bom":          empty_worker_cluster(),
                    "cmh":          cluster(consume_replicas=3),
                },
                prod={
                    "northwest":    cluster(consume_replicas=8,
                                            fanout_replicas=4,
                                            mb_per_worker=2600),
                    "bom":          cluster(consume_replicas=2,
                                            fanout_replicas=2),
                    "cmh":          cluster(consume_replicas=16,
                                            fanout_replicas=6,
                                            mb_per_worker=2600),
                }),
        hostset("capital",
                suffix="low-cpu",
                default=            cluster(mb_per_worker=2500),
                qa={
                    "northwest":    cluster(),
                    "bom":          cluster(),
                    "cmh":          cluster(),
                },
                preprod={
                    "northwest":    empty_worker_cluster(),
                    "bom":          empty_worker_cluster(),
                    "cmh":          cluster(),
                },
                prod={
                    "northwest":    cluster(consume_replicas=4, mb_per_worker=2600),
                    "bom":          cluster(),
                    "cmh":          cluster(consume_replicas=4, mb_per_worker=2600),
                }),
        hostset("cards-authz",
                suffix="low-cpu",
                disable_preloading=True,
                default=            cluster(mb_per_worker=3000),
                qa={
                    "northwest":    cluster(),
                    "bom":          cluster(),
                    "cmh":          cluster(),
                },
                preprod={
                    "northwest":    cluster(),
                    "bom":          cluster(),
                    "cmh":          cluster(),
                },
                prod={
                    "northwest":    cluster(consume_replicas=12,
                                            fanout_replicas=4,
                                            mb_per_worker=2000),
                    "bom":          cluster(),
                    "cmh":          cluster(consume_replicas=12,
                                            fanout_replicas=4,
                                            mb_per_worker=2000),
                }),
        hostset("clearing",
                disable_preloading=True, # This allows a smaller mb_per_worker. If re-enabling preloading, make sure to bump mb_per_worker
                default=            cluster(mb_per_worker=3100),
                qa={
                    "northwest":    cluster(),
                    "bom":          cluster(),
                    "cmh":          cluster(),
                },
                prod={
                    "northwest":    cluster(consume_replicas=34,
                                            fanout_replicas=4,
                                            mb_per_worker=3200,
                                            instance_type=m6g_16xlarge),
                    "bom":          cluster(mb_per_worker=2700,
                                            consume_replicas=6),
                    "cmh":          cluster(consume_replicas=10,
                                            fanout_replicas=4,
                                            mb_per_worker=3200,
                                            instance_type=m6g_16xlarge),
                }),
        hostset("connect",
                suffix="low-cpu",
                disable_preloading=True,
                qa={
                    "northwest":    cluster(mb_per_worker=2800),
                    "bom":          cluster(),
                    "cmh":          cluster(),
                },
                preprod={
                    "northwest":    empty_worker_cluster(),
                    "bom":          empty_worker_cluster(),
                    "cmh":          cluster(),
                },
                prod={
                    "northwest":    cluster(consume_replicas=8,
                                            mb_per_worker=2000),
                    "bom":          cluster(),
                    "cmh":          cluster(consume_replicas=8,
                                            mb_per_worker=2000),
                }),
        hostset("connections",
                suffix="low-cpu",
                default=            cluster(mb_per_worker=2800),
                qa={
                    "northwest":    cluster(),
                    "bom":          cluster(),
                    "cmh":          cluster(),
                },
                preprod={
                    "northwest":    empty_worker_cluster(),
                    "bom":          empty_worker_cluster(),
                    "cmh":          cluster(),
                },
                prod={
                    "northwest":    cluster(consume_replicas=6,
                                            fanout_replicas=3),
                    "bom":          cluster(consume_replicas=3,
                                            fanout_replicas=3),
                    "cmh":          cluster(consume_replicas=6,
                                            fanout_replicas=3),
                }),
        hostset("consumer-intel",
                suffix="low-cpu",
                default=            cluster(mb_per_worker=3000),
                qa={
                    "northwest":    cluster(graviton_migration_enabled=True, graviton_migration_size="100%", graviton_migration_target_6g=True),
                    "bom":          empty_worker_cluster(),
                    "cmh":          cluster(graviton_migration_enabled=True, graviton_migration_size="100%", graviton_migration_target_6g=True),
                },
                preprod={
                    "northwest":    empty_worker_cluster(),
                    "bom":          empty_worker_cluster(),
                    "cmh":          empty_worker_cluster(),
                },
                prod={
                    "northwest":    cluster(consume_replicas=4,
                                            fanout_replicas=4,
                                            graviton_migration_enabled=True,
                                            graviton_migration_size="100%",
                                            graviton_migration_target_6g=True),
                    "bom":          empty_worker_cluster(),
                    "cmh":          cluster(consume_replicas=4,
                                            fanout_replicas=4,
                                            graviton_migration_enabled=True,
                                            graviton_migration_size="100%",
                                            graviton_migration_target_6g=True),
                }),
        hostset("emails",
                suffix="low-cpu",
                default=            cluster(mb_per_worker=3000),
                qa={
                    "northwest":    cluster(),
                    "bom":          cluster(),
                    "cmh":          cluster(),
                },
                preprod={
                    "northwest":    empty_worker_cluster(),
                    "bom":          empty_worker_cluster(),
                    "cmh":          cluster(consume_replicas=3,
                                            instance_type=r7g_xlarge),
                },
                prod={
                    "northwest":    cluster(mb_per_worker=2800,
                                            consume_replicas=9,
                                            fanout_replicas=4,
                                            instance_type=r7g_8xlarge),
                    "bom":          cluster(consume_replicas=6),
                    "cmh":          cluster(mb_per_worker=2800,
                                            consume_replicas=9,
                                            fanout_replicas=4,
                                            instance_type=r7g_8xlarge),
                }),
        hostset("high_priority_emails",
                suffix="low-cpu",
                default=            cluster(mb_per_worker=3000),
                qa={
                    "northwest":    cluster(),
                    "bom":          cluster(),
                    "cmh":          cluster(),
                },
                preprod={
                    "northwest":    empty_worker_cluster(),
                    "bom":          empty_worker_cluster(),
                    "cmh":          cluster(instance_type=r7g_xlarge),
                },
                prod={
                    "northwest":    cluster(fanout_replicas=4,
                                            instance_type=r7g_8xlarge),
                    "bom":          cluster(),
                    "cmh":          cluster(fanout_replicas=4,
                                            instance_type=r7g_8xlarge),
                }),
        hostset("frontline_experience",
                suffix="low-cpu",
                default=            cluster(mb_per_worker=2875),
                qa={
                    "northwest":    cluster(),
                    "bom":          cluster(),
                    "cmh":          cluster(),
                },
                preprod={
                    "northwest":    empty_worker_cluster(),
                    "bom":          empty_worker_cluster(),
                    "cmh":          cluster(),
                },
                prod={
                    "northwest":    cluster(consume_replicas=4),
                    "bom":          cluster(consume_replicas=3),
                    "cmh":          cluster(consume_replicas=4),
                }),
        hostset("issuing",
                suffix="low-cpu",
                default=            cluster(mb_per_worker=3300),
                qa={
                    "northwest":    cluster(mb_per_worker=3300),
                    "bom":          cluster(mb_per_worker=6300),
                    "cmh":          cluster(mb_per_worker=6300),
                },
                preprod={
                    "northwest":    empty_worker_cluster(),
                    "bom":          empty_worker_cluster(),
                    "cmh":          cluster(mb_per_worker=6300, instance_type=r7g_xlarge),
                },
                prod={
                    "northwest":    cluster(mb_per_worker=3650,
                                            consume_replicas=18,
                                            fanout_replicas=5,
                                            instance_type=r7g_8xlarge),
                    "bom":          cluster(mb_per_worker=6300,
                                            consume_replicas=6,
                                            fanout_replicas=6),
                    "cmh":          cluster(mb_per_worker=3650,
                                            consume_replicas=18,
                                            fanout_replicas=5,
                                            instance_type=r7g_8xlarge),
                    }),
        hostset("local-payment-methods",
                suffix="low-cpu",
                default=cluster(mb_per_worker=2400),
                qa={
                    "northwest":    cluster(),
                    "bom":          cluster(),
                    "cmh":          cluster(),
                },
                preprod={
                    "northwest":    empty_worker_cluster(),
                    "bom":          empty_worker_cluster(),
                    "cmh":          cluster(),
                },
                prod={
                    "northwest":    cluster(mb_per_worker=2800,
                                            consume_replicas=9),
                    "bom":          cluster(),
                    "cmh":          cluster(mb_per_worker=2600,
                                            consume_replicas=8),
                }),
        hostset("merchant-fraud-ml",
                disable_preloading=True, # This allows a smaller mb_per_worker. If re-enabling preloading, make sure to bump mb_per_worker
                default=            cluster(mb_per_worker=2400),
                qa={
                    "northwest":    cluster(mb_per_worker=2800),
                    "bom":          cluster(mb_per_worker=2800),
                    "cmh":          cluster(mb_per_worker=2800),
                },
                prod={
                    "northwest":    cluster(consume_replicas=28,
                                            fanout_replicas=10,
                                            mb_per_worker=1700,
                                            instance_type=m6g_16xlarge # This hostset gets CPU bound when running on r7g_8xlarge, so going to a higher CPU instance
                                            ),
                    "bom":          cluster(consume_replicas=4),
                    "cmh":          cluster(consume_replicas=28,
                                            fanout_replicas=10,
                                            mb_per_worker=1700,
                                            instance_type=m6g_16xlarge),
                }),
        hostset("mobile",
                suffix="low-cpu",
                default=            cluster(mb_per_worker=4500),
                qa={
                    "northwest":    cluster(),
                    "bom":          cluster(),
                    "cmh":          cluster(),
                },
                preprod={
                    "northwest":    empty_worker_cluster(),
                    "bom":          empty_worker_cluster(),
                    "cmh":          cluster(),
                },
                prod={
                    "northwest":    cluster(consume_replicas=12),
                    "bom":          cluster(),
                    "cmh":          cluster(consume_replicas=12),
                }),
        hostset("latam",
                suffix="low-cpu",
                qa=all_regions_empty_worker_cluster(),
                prod=all_regions_empty_worker_cluster(),
                ),
        hostset("link",
                suffix="low-cpu",
                default=            cluster(mb_per_worker=2600),
                preprod={
                    "northwest":    cluster(consume_replicas=4),
                    "cmh":          cluster(consume_replicas=3),
                },
                prod={
                    "northwest":    cluster(consume_replicas=4,
                                            fanout_replicas=3),
                    "cmh":          cluster(consume_replicas=4,
                                            fanout_replicas=3),
                }),
        hostset("link-preference",
            suffix="low-cpu",
            qa={
                "northwest":    cluster(graviton_migration_enabled=True, graviton_migration_size="100%", graviton_migration_target_6g=True),
                "bom":          empty_worker_cluster(),
                "cmh":          empty_worker_cluster(),
            },
            preprod={
                "northwest":    cluster(graviton_migration_enabled=True, graviton_migration_size="100%", graviton_migration_target_6g=True),
                "bom":          empty_worker_cluster(),
                "cmh":          empty_worker_cluster(),
            },
            prod={
                "northwest":    cluster(graviton_migration_enabled=True, graviton_migration_size="100%", graviton_migration_target_6g=True),
                "bom":          empty_worker_cluster(),
                "cmh":          empty_worker_cluster(),
            },
        ),
        hostset("model-ingestion",
                suffix="low-cpu",
                # pre-loading disabled for fanout workers through environment variables
                prod={
                    "northwest":    cluster(consume_replicas=70, # autoscaling below
                                            fanout_replicas=18,
                                            mb_per_worker=2900,
                                            instance_type=r6g_8xlarge),
                    "bom":          cluster(consume_replicas=6,  # autoscaling below
                                            fanout_replicas=2,
                                            instance_type=r6g_xlarge),
                    "cmh":          cluster(consume_replicas=70, # autoscaling below
                                            fanout_replicas=18,
                                            mb_per_worker=2900,
                                            instance_type=r6g_8xlarge),
                },
                qa={
                    "northwest":    cluster(),
                    "bom":          cluster(),
                    "cmh":          cluster(),
                },
                autoscaling={
                    "prod": {
                        "northwest": {
                            "consume": autoscaling_config(
                                host_set = "model-ingestion",
                                service = "workers",
                                cluster = "northwest",
                                availability_threshold = "0.65",
                                cpu_threshold = 80,
                                memory_threshold = 90,
                                min_replicas= 20,
                                max_replicas= 70,
                                scale_up_stabilization_window_seconds = 0, # Make the decision to scale immediately, without waiting for a window
                                scale_up_policy = scale_policy(
                                    policy_type = "Percent", # Scale up at most by 100% of the current count of replicas, each period
                                    value = 100,
                                    period_seconds = 60, # Scale up at most every 60 seconds
                                ),
                                scale_down_stabilization_window_seconds = 300, # The highest desired state from the past previous 5 minute window will be the new scaling target
                                scale_down_policy = scale_policy(
                                    policy_type = "Pods", # Scale down at most by 2 replicas, each period
                                    value = 2,
                                    period_seconds = 300, # Scale down at most every 5 minutes
                                ),
                                enable_immediate_scaling = True,
                            ),
                        },
                        "cmh": {
                            "consume": autoscaling_config(
                                host_set = "model-ingestion",
                                service = "workers",
                                cluster = "cmh",
                                availability_threshold = "0.65",
                                cpu_threshold = 80,
                                memory_threshold = 90,
                                min_replicas= 20,
                                max_replicas= 70,
                                scale_up_stabilization_window_seconds = 0,
                                scale_up_policy = scale_policy(
                                    policy_type = "Percent",
                                    value = 100,
                                    period_seconds = 60,
                                ),
                                scale_down_stabilization_window_seconds = 300,
                                scale_down_policy = scale_policy(
                                    policy_type = "Pods",
                                    value = 2,
                                    period_seconds = 300,
                                ),
                                enable_immediate_scaling = True,
                            ),
                        },
                        "bom": {
                            "consume": autoscaling_config(
                                host_set = "model-ingestion",
                                service = "workers",
                                cluster = "bom",
                                availability_threshold = "0.65",
                                cpu_threshold = 80,
                                memory_threshold = 90,
                                min_replicas= 2,
                                max_replicas= 30,
                                scale_up_stabilization_window_seconds = 0,
                                scale_up_policy = scale_policy(
                                    policy_type = "Percent",
                                    value = 100,
                                    period_seconds = 60,
                                ),
                                scale_down_stabilization_window_seconds = 300,
                                scale_down_policy = scale_policy(
                                    policy_type = "Pods",
                                    value = 2,
                                    period_seconds = 300,
                                ),
                                enable_immediate_scaling = True,
                            ),
                        }
                    }
                }),
        hostset("payment-flows",
                suffix="low-cpu",
                default=            cluster(mb_per_worker=2600),
                qa={
                    "northwest":    cluster(),
                    "bom":          cluster(),
                    "cmh":          cluster(),
                },
                preprod={
                    "northwest":    cluster(consume_replicas=4),
                    "cmh":          cluster(consume_replicas=4),
                },
                prod={
                    "northwest":    cluster(consume_replicas=4,
                                            fanout_replicas=3),
                    "bom":          cluster(),
                    "cmh":          cluster(consume_replicas=8,
                                            fanout_replicas=3),
                }),
        hostset("payouts",
                suffix="low-cpu",
                disable_preloading=True, # This allows a smaller mb_per_worker. If re-enabling preloading, make sure to bump mb_per_worker
                default=            cluster(mb_per_worker=3100),
                qa={
                    "northwest":    cluster(consume_replicas=12,
                                            fanout_replicas=4,
                                            mb_per_worker=2800),
                    "bom":          cluster(mb_per_worker=2800),
                    "cmh":          cluster(mb_per_worker=2800),
                },
                prod={
                    "northwest":    cluster(consume_replicas=24,
                                            fanout_replicas=6,
                                            mb_per_worker=2100,
                                            instance_type=r7g_8xlarge),
                    "bom":          cluster(consume_replicas=6,
                                            fanout_replicas=4,
                                            mb_per_worker=4500),
                    "cmh":          cluster(consume_replicas=24,
                                            fanout_replicas=6,
                                            mb_per_worker=2100,
                                            instance_type=r7g_8xlarge),


                }),
        hostset("radar",
                suffix="low-cpu",
                disable_preloading=True,
                qa={
                    "northwest":    cluster(),
                    "bom":          cluster(),
                    "cmh":          cluster(),
                },
                preprod={
                    "northwest":    empty_worker_cluster(),
                    "bom":          empty_worker_cluster(),
                    "cmh":          cluster(),
                },
                prod={
                    "northwest": cluster(consume_replicas=12,
                                         fanout_replicas=4,
                                         mb_per_worker=1100),
                    "bom":       cluster(consume_replicas=6,
                                         fanout_replicas=3,
                                         mb_per_worker=2500),
                    "cmh":       cluster(consume_replicas=10,
                                         fanout_replicas=4,
                                         mb_per_worker=1200),
                }),
        hostset("review-platform",
                suffix="low-cpu",
                qa={
                    "northwest":    cluster(),
                    "bom":          cluster(),
                    "cmh":          cluster(),
                },
                preprod={
                    "northwest":    empty_worker_cluster(),
                    "bom":          empty_worker_cluster(),
                    "cmh":          cluster(),
                },
                prod={
                    "northwest":    cluster(consume_replicas=6,
                                            mb_per_worker=2600),
                    "bom":          cluster(),
                    "cmh":          cluster(consume_replicas=6,
                                            mb_per_worker=2600),
                }),
        hostset("revolve-default",
                disable_preloading=True,
                default=            cluster(mb_per_worker=2500),
                qa={
                    "northwest":    cluster(),
                    "bom":          cluster(),
                    "cmh":          cluster(),
                },
                preprod={
                    "northwest":    empty_worker_cluster(),
                    "bom":          empty_worker_cluster(),
                    "cmh":          cluster(consume_replicas=3),
                },
                prod={
                    # Please reach out to #billing-infrastructure before changing this config!
                    # We have large traffic spikes around end of month and we have carefully chosen config
                    # to support this
                    "northwest":    cluster(consume_replicas=20, # This is scaled on a scheduled basis, see autoscaling config below
                                            fanout_replicas=2,
                                            mb_per_worker=2000,
                                            instance_type=m6g_16xlarge),
                    "bom":    cluster(consume_replicas=10,
                                                fanout_replicas=2,
                                            instance_type=r6g_xlarge),
                    "cmh":          cluster(consume_replicas=10, # This is scaled on a scheduled basis, see autoscaling config below
                                            fanout_replicas=2,
                                            mb_per_worker=2000,
                                            instance_type=m6g_16xlarge),
                },
                autoscaling={
                    "prod": {
                        "cmh": {
                            "consume": scaled_object(
                                schedule_triggers= [
                                    # Trigger for 31-day months (January, March, May, July, August, October, December)
                                    schedule_trigger(
                                        start= "0 23 24 1,3,5,7,8,10,12 *",  # Start scaling at 23:00 on the 24th for 31-day months
                                        end= "0 12 2 * *",                   # Scale down at 12:00 on the 2nd of any next month
                                        desired_replicas= 31,
                                        timezone= "UTC"
                                    ),

                                    # Trigger for 30-day months (April, June, September, November)
                                    schedule_trigger(
                                        start= "0 23 23 4,6,9,11 *",  # Start scaling at 23:00 on the 23rd for 30-day months
                                        end= "0 12 2 * *",            # Scale down at 12:00 on the 2nd of any next month
                                        desired_replicas= 31,
                                        timezone= "UTC"
                                    ),

                                    # Trigger for February (assumed to always be 28 days)
                                    # Note: In leap years, February can have 29 days, but this configuration assumes a standard 28-day month.
                                    schedule_trigger(
                                        start= "0 23 22 2 *",       # Start scaling at 23:00 on the 22nd specifically for February
                                        end= "0 12 2 * *",          # Scale down at 12:00 on the 2nd of any next month
                                        desired_replicas= 31,
                                        timezone= "UTC"
                                    ),
                                ],
                                min_replicas = 10,
                                max_replicas = 31,
                                enable_immediate_scaling = True,
                            )
                        },
                        "northwest": {
                            "consume": scaled_object(
                                schedule_triggers= [
                                    # Trigger for 31-day months (January, March, May, July, August, October, December)
                                    schedule_trigger(
                                        start= "0 23 24 1,3,5,7,8,10,12 *",  # Start scaling at 23:00 on the 24th for 31-day months
                                        end= "0 12 2 * *",                   # Scale down at 12:00 on the 2nd of any next month
                                        desired_replicas= 31,
                                        timezone= "UTC"
                                    ),

                                    # Trigger for 30-day months (April, June, September, November)
                                    schedule_trigger(
                                        start= "0 23 23 4,6,9,11 *",  # Start scaling at 23:00 on the 23rd for 30-day months
                                        end= "0 12 2 * *",            # Scale down at 12:00 on the 2nd of any next month
                                        desired_replicas= 31,
                                        timezone= "UTC"
                                    ),

                                    # Trigger for February (assumed to always be 28 days)
                                    # Note: In leap years, February can have 29 days, but this configuration assumes a standard 28-day month.
                                    schedule_trigger(
                                        start= "0 23 22 2 *",       # Start scaling at 23:00 on the 22nd specifically for February
                                        end= "0 12 2 * *",          # Scale down at 12:00 on the 2nd of any next month
                                        desired_replicas= 31,
                                        timezone= "UTC"
                                    ),
                                ],
                                min_replicas = 20,
                                max_replicas = 31,
                                enable_immediate_scaling = True,
                            )
                        }
                    },
                }),
        hostset("risk",
                suffix="low-cpu",
                disable_preloading=True, # This allows a smaller mb_per_worker. If re-enabling preloading, make sure to bump mb_per_worker
                qa={
                    "northwest":    cluster(),
                    "bom":          cluster(),
                    "cmh":          cluster(),
                },
                preprod={
                    "northwest":    empty_worker_cluster(),
                    "bom":          empty_worker_cluster(),
                    "cmh":          cluster(consume_replicas=6,
                                            fanout_replicas=4,
                                            instance_type=r7g_xlarge),
                },
                prod={
                    "northwest":    cluster(mb_per_worker=1800,
                                            consume_replicas=16,
                                            fanout_replicas=4,
                                            instance_type=r7g_8xlarge),
                    "bom":          cluster(mb_per_worker=1600,
                                            consume_replicas=6),
                    "cmh":          cluster(mb_per_worker=1600,
                                            consume_replicas=11,
                                            fanout_replicas=4,
                                            instance_type=r7g_8xlarge),
                }),
        hostset("schema-ci-status-consumer",
                suffix="low-cpu",
                qa={
                    "northwest":    cluster(),
                    "bom":          empty_worker_cluster(),
                    "cmh":          empty_worker_cluster(),
                },
                prod={
                    "northwest":    cluster(),
                    "bom":          empty_worker_cluster(),
                    "cmh":          cluster(),
                },
        ),
        hostset("search-desc-connect",
                disable_preloading=True, # This allows a smaller mb_per_worker. If re-enabling preloading, make sure to bump mb_per_worker
                qa={
                    "northwest":    cluster(mb_per_worker=2800),
                    "bom":          cluster(mb_per_worker=2800),
                    "cmh":          cluster(mb_per_worker=2800),
                },
                prod={
                    "northwest":    cluster(mb_per_worker=1900,
                                            consume_replicas=15,
                                            fanout_replicas=2,
                                            instance_type=m6g_16xlarge), # This hostset gets CPU bound when running on the default r6g_8xlarge, so going to a higher CPU instance
                    "bom":          cluster(mb_per_worker=1850,
                                            consume_replicas=6,
                                            fanout_replicas=2),
                    "cmh":          cluster(mb_per_worker=2800,
                                            consume_replicas=15,
                                            fanout_replicas=2,
                                            instance_type=m6g_16xlarge),
                }),
        hostset("sigma-assistant",
                suffix="low-cpu",
                disable_preloading=True, # This allows a smaller mb_per_worker. If re-enabling preloading, make sure to bump mb_per_worker
                qa=                 all_regions_empty_worker_cluster(),
                preprod =           all_regions_empty_worker_cluster(),
                prod =              all_regions_empty_worker_cluster(),
        ),
        hostset("tax-product",
            suffix="low-cpu",
            default=            cluster(mb_per_worker=2500),
            qa={
                "northwest":    cluster(),
                "bom":          cluster(),
                "cmh":          cluster(),
            },
            prod={
                "northwest":    cluster(),
                "bom":          cluster(),
                "cmh":          cluster(),
            },
        ),
        hostset("verifications",
                suffix="low-cpu",
                disable_preloading=True,
                default=            cluster(mb_per_worker=2800),
                qa={
                    "northwest":    cluster(consume_replicas=6),
                    "bom":          cluster(),
                    "cmh":          cluster(),
                },
                preprod={
                    "northwest":    empty_worker_cluster(),
                    "bom":          empty_worker_cluster(),
                    # TODO(RUN_IDCORE, 2022-12-01): Investigate whether the cmh
                    # entry here can be removed. See
                    # https://docs.google.com/document/d/1xVENl7Kxtkz2V3fHHnMNjbmTet9z9ofAYLI_L32lKm8/edit
                    # for details.
                    "cmh":          cluster(consume_replicas=4),
                },
                prod={
                    "northwest":    cluster(consume_replicas=13,
                                            fanout_replicas=4),
                    "bom":          cluster(consume_replicas=2,
                                            fanout_replicas=2),
                    "cmh":          cluster(consume_replicas=13,
                                            fanout_replicas=4),
                }),
        hostset("webhooks",
                is_canary=True,
                disable_preloading=True, # If re-enabling preloading, make sure to bump mb_per_worker to 2400 and scale accordingly
                suffix="low-cpu",
                qa= {
                    "northwest":    cluster(mb_per_worker=2500, instance_type=r7g_xlarge),
                    "bom":          cluster(),
                    "cmh":          cluster(instance_type=r7g_xlarge),
                },
                prod={
                    "northwest":    cluster(consume_replicas=14,
                                            fanout_replicas=3,
                                            mb_per_worker=900, # 263 workers
                                            instance_type=r7g_8xlarge),
                    "bom":          cluster(consume_replicas=10,
                                            mb_per_worker=750),
                    "cmh":          cluster(consume_replicas=14,
                                            fanout_replicas=3,
                                            mb_per_worker=900, # 263 workers
                                            instance_type=r7g_8xlarge),
                }
        ),
        hostset("webhooks",
                is_canary=False,
                disable_preloading=True, # If re-enabling preloading, make sure to bump mb_per_worker to 2400 and scale accordingly
                suffix="low-cpu",
                qa= {
                    "northwest":    cluster(mb_per_worker=2500, instance_type=r7g_xlarge),
                    "bom":          cluster(),
                    "cmh":          cluster(instance_type=r7g_xlarge),
                },
                prod={
                    "northwest":    cluster(consume_replicas=62,
                                            fanout_replicas=9,
                                            mb_per_worker=900, # 263 workers
                                            instance_type=r7g_8xlarge),
                    "bom":          cluster(consume_replicas=30,
                                            fanout_replicas=3,
                                            mb_per_worker=750),
                    "cmh":          cluster(consume_replicas=40,
                                            fanout_replicas=7,
                                            mb_per_worker=900, # 263 workers
                                            instance_type=r7g_8xlarge),
                }
        ),
    ]
)

A400_HOST_SETS = struct(
    availability_tier=AVAILABILITY_TIER_A400,
    hostsets=[
        hostset("general",
                default=            cluster(mb_per_worker=4000),
                qa={
                    "bom":          cluster(consume_replicas=2,
                                            fanout_replicas=2,
                                            instance_type=r6g_8xlarge),
                    "cmh":          cluster(consume_replicas=2,
                                            fanout_replicas=2,
                                            instance_type=r6g_8xlarge),
                    "northwest":    cluster(consume_replicas=6,
                                            fanout_replicas=3,
                                            instance_type=r6g_8xlarge),
                },
                prod={
                    "cmh":          cluster(mb_per_worker=4800,
                                            consume_replicas=400,
                                            fanout_replicas=0,
                                            instance_type=m6g_16xlarge),
                    "northwest":    cluster(mb_per_worker=4800, # Increasing since memory utilization is hovering around ~90%
                                            consume_replicas=400,
                                            fanout_replicas=0,
                                            instance_type=m6g_16xlarge),
                    "bom":          empty_worker_cluster(),
                },
                autoscaling={
                    "qa": {
                        "northwest": {
                            "consume": autoscaling_config(
                                host_set = "general",
                                service = "workers",
                                cluster = "northwest",
                                availability_threshold = "0.50", # Scale up when we cross 50% worker availability
                                cpu_threshold = 50, # Scale up when we cross 50% CPU Utilization
                                memory_threshold = 80, # Scale up when we cross 80% Memory utilization
                                min_replicas= 3,
                                max_replicas= 6,
                                scale_up_stabilization_window_seconds = 0, # Make the decision to scale immediately, without waiting for a window
                                scale_up_policy = scale_policy(
                                    policy_type = "Percent", # Scale up by 100% of the current count of replicas every 60 seconds
                                    value = 100,
                                    period_seconds = 60, # Add at most 2x current replica count every 60 seconds
                                ),
                                scale_down_stabilization_window_seconds = 300, # The highest desired state from the past previous 5 minute window will be the new scaling target
                                scale_down_policy = scale_policy(
                                    policy_type = "Percent", # Scale down by 25% of the current count of replicas every 60 seconds
                                    value = 25,
                                    period_seconds = 300, # Remove at most 25% of current pods every 5 minutes
                                ),
                                enable_immediate_scaling = True,
                            ),
                        },
                        "bom": {
                            "consume": autoscaling_config(
                                host_set = "general",
                                service = "workers",
                                cluster = "bom",
                                availability_threshold = "0.50", # Scale up when we cross 50% worker availability
                                cpu_threshold = 50, # Scale up when we cross 50% CPU Utilization
                                memory_threshold = 80, # Scale up when we cross 80% Memory utilization
                                min_replicas= 2, # 100% of bom replicas
                                max_replicas= 4,
                                scale_up_stabilization_window_seconds = 0, # Make the decision to scale immediately, without waiting for a window
                                scale_up_policy = scale_policy(
                                    policy_type = "Percent", # Scale up by 100% of the current count of replicas every 60 seconds
                                    value = 100,
                                    period_seconds = 60, # Add at most 2x current replica count every 60 seconds
                                ),
                                scale_down_stabilization_window_seconds = 300, # The highest desired state from the past previous 5 minute window will be the new scaling target
                                scale_down_policy = scale_policy(
                                    policy_type = "Percent", # Scale down by 25% of the current count of replicas every 60 seconds
                                    value = 25,
                                    period_seconds = 300, # Remove at most 25% of current pods every 5 minutes
                                ),
                                enable_immediate_scaling = True,
                            ),
                        },
                        "cmh": {
                            "consume": autoscaling_config(
                                host_set = "general",
                                service = "workers",
                                cluster = "cmh",
                                availability_threshold = "0.50", # Scale up when we cross 50% worker availability
                                cpu_threshold = 50, # Scale up when we cross 50% CPU Utilization
                                memory_threshold = 80, # Scale up when we cross 80% Memory utilization
                                min_replicas= 2,
                                max_replicas= 4,
                                scale_up_stabilization_window_seconds = 0, # Make the decision to scale immediately, without waiting for a window
                                scale_up_policy = scale_policy(
                                    policy_type = "Percent", # Scale up by 100% of the current count of replicas every 60 seconds
                                    value = 100,
                                    period_seconds = 60, # Add at most 2x current replica count every 60 seconds
                                ),
                                scale_down_stabilization_window_seconds = 300, # The highest desired state from the past previous 5 minute window will be the new scaling target
                                scale_down_policy = scale_policy(
                                    policy_type = "Percent", # Scale down by 25% of the current count of replicas every 60 seconds
                                    value = 25,
                                    period_seconds = 300, # Remove at most 25% of current pods every 5 minutes
                                ),
                                enable_immediate_scaling = True,
                            ),
                        }
                    },
                    "prod": {
                        "northwest": {
                            "consume": autoscaling_config(
                                host_set = "general",
                                service = "workers",
                                cluster = "northwest",
                                availability_threshold = "0.50", # Scale up when we cross 50% worker availability
                                cpu_threshold = 50, # Scale up when we cross 50% CPU Utilization
                                memory_threshold = 90, # Scale up when we cross 90% Memory utilization
                                min_replicas= 356, # 95% of northwest replicas
                                max_replicas= 400,
                                scale_up_stabilization_window_seconds = 0, # Make the decision to scale immediately, without waiting for a window
                                scale_up_policy = scale_policy(
                                    policy_type = "Percent", # Scale up by 100% of the current count of replicas every 60 seconds
                                    value = 100,
                                    period_seconds = 60, # Add at most 2x current replica count every 60 seconds
                                ),
                                scale_down_stabilization_window_seconds = 300, # The highest desired state from the past previous 5 minute window will be the new scaling target
                                scale_down_policy = scale_policy(
                                    policy_type = "Percent", # Scale down by 25% of the current count of replicas every 60 seconds
                                    value = 25,
                                    period_seconds = 300, # Remove at most 25% of current pods every 5 minutes
                                ),
                                enable_immediate_scaling = True,
                            ),
                        },
                        "cmh": {
                            "consume": autoscaling_config(
                                host_set = "general",
                                service = "workers",
                                cluster = "cmh",
                                availability_threshold = "0.50", # Scale up when we cross 50% worker availability
                                cpu_threshold = 50, # Scale up when we cross 50% CPU Utilization
                                memory_threshold = 80, # Scale up when we cross 80% Memory utilization
                                min_replicas= 200, # 50% of cmh replicas
                                max_replicas= 400,
                                scale_up_stabilization_window_seconds = 0, # Make the decision to scale immediately, without waiting for a window
                                scale_up_policy = scale_policy(
                                    policy_type = "Percent", # Scale up by 100% of the current count of replicas every 60 seconds
                                    value = 100,
                                    period_seconds = 60, # Add at most 2x current replica count every 60 seconds
                                ),
                                scale_down_stabilization_window_seconds = 300, # The highest desired state from the past previous 5 minute window will be the new scaling target
                                scale_down_policy = scale_policy(
                                    policy_type = "Percent", # Scale down bt 25% of the current count of replicas every 60 seconds
                                    value = 25,
                                    period_seconds = 300, # Remove at most 25% of current pods every 5 minutes
                                ),
                                enable_immediate_scaling = True,
                            ),
                        }
                    }
                },
        ),
        hostset("general",
                suffix="low-cpu",
                default=            cluster(mb_per_worker=4000),
                qa=all_regions_empty_worker_cluster(),
                preprod={
                    "northwest":    empty_worker_cluster(),
                    "bom":          empty_worker_cluster(),
                    "cmh":          cluster(consume_replicas=5,
                                            fanout_replicas=3,
                                            instance_type=r6g_8xlarge),
                },
                prod={
                    "cmh":          cluster(mb_per_worker=3800,
                                            consume_replicas=0,
                                            fanout_replicas=40,
                                            instance_type=r6g_8xlarge),
                    "bom":          cluster(mb_per_worker=3600,
                                            consume_replicas=6,
                                            fanout_replicas=3,
                                            instance_type=r6g_8xlarge),
                    # pre-loading disabled for fanout workers through environment variables
                    # general workers need a little bit more cpu, so we're increasing the
                    # mb_per_worker to reduce the number of workers per pod
                    "northwest":    cluster(mb_per_worker=4200, # # Increasing since memory utilization is hovering > ~90% and worker utilizatio is still low.
                                            consume_replicas=0,
                                            fanout_replicas=70),
                },
                autoscaling={
                    "prod": {
                        "bom": {
                            "consume": autoscaling_config(
                                host_set = "general",
                                service = "workers",
                                cluster = "bom",
                                availability_threshold = "0.50", # Scale up when we cross 50% worker availability
                                cpu_threshold = 50, # Scale up when we cross 50% CPU Utilization
                                memory_threshold = 80, # Scale up when we cross 80% Memory utilization
                                min_replicas= 3,
                                max_replicas= 8,
                                scale_up_stabilization_window_seconds = 0, # Make the decision to scale immediately, without waiting for a window
                                scale_up_policy = scale_policy(
                                    policy_type = "Percent", # Scale up by 100% of the current count of replicas every 60 seconds
                                    value = 100,
                                    period_seconds = 60, # Add at most 2x current replica count every 60 seconds
                                ),
                                scale_down_stabilization_window_seconds = 300, # The highest desired state from the past previous 5 minute window will be the new scaling target
                                scale_down_policy = scale_policy(
                                    policy_type = "Percent", # Scale down by 25% of the current count of replicas every 60 seconds
                                    value = 25,
                                    period_seconds = 300, # Remove at most 25% of current pods every 5 minutes
                                ),
                                enable_immediate_scaling = True,
                            ),
                        },
                    },
                }),
    ]
)

# Sigma used to use autoscaling, thus the "auto" in msp-a400-auto
# Details on how to restore it in the future: https://jira.corp.stripe.com/browse/ASYNCPROCESSING-2141
SIGMA_HOST_SET = struct(
    availability_tier=AVAILABILITY_TIER_A400_AUTO,
    hostsets=[
        hostset("sigma",
                # N.B: the math below for the number of workers is the original sizing;
                # this is a temporary increase in memory to alleviate https://go/ir/desk-subside
                default=            cluster(mb_per_worker=2500),
                qa={
                    "bom":    cluster(consume_replicas=2, fanout_replicas=2),
                    "cmh":          cluster(consume_replicas=0,
                                            fanout_replicas=0),
                    "northwest":    cluster(mb_per_worker=2900,
                                            consume_replicas=2), # autoscaling below
                },
                # pre-loading disabled for fanout workers through environment variables
                prod={
                    "cmh":          cluster(consume_replicas=0,
                                            fanout_replicas=0),
                    "northwest":    cluster(consume_replicas=300, # autoscaling below
                                            fanout_replicas=15,
                                            mb_per_worker=3100),
                    "bom":
                    # As part of the cutover for India DL, we expect BOM to see up to 20% of northwest's traffic
                    # Since we use the same spot instance mix in both regions, we can simply size to 20% of northwest
                    # At max, we'll have 50 consumer worker pods (1 per node) x 72 workers/pod = 3,600 workers
                                    cluster(consume_replicas=30, # autoscaling below
                                            fanout_replicas=3),
                },
                autoscaling={
                    "qa": {
                        "northwest": {
                            "consume": autoscaling_config(
                                host_set = "sigma",
                                service = "workers",
                                cluster = "northwest",
                                availability_threshold = "0.90",
                                cpu_threshold = 90,
                                memory_threshold = 90,
                                min_replicas= 2,
                                max_replicas = 4,
                                scale_up_stabilization_window_seconds = 0, # Make the decision to scale immediately, without waiting for a window
                                scale_up_policy = scale_policy(
                                    policy_type = "Percent", # Scale up at most by 100% of the current count of replicas, each period
                                    value = 100,
                                    period_seconds = 60, # Scale up at most every 60 seconds
                                ),
                                scale_down_stabilization_window_seconds = 300, # The highest desired state from the past previous 5 minute window will be the new scaling target
                                scale_down_policy = scale_policy(
                                    policy_type = "Percent", # Scale down at most by 25% of the current count of replicas, each period
                                    value = 25,
                                    period_seconds = 300, # Scale down at most every 5 minutes
                                ),
                                enable_immediate_scaling = True,
                            ),
                        }
                    },
                    "prod": {
                        "northwest": {
                            "consume": autoscaling_config(
                                host_set = "sigma",
                                service = "workers",
                                cluster = "northwest",
                                availability_threshold = "0.60",
                                cpu_threshold = 80,
                                memory_threshold = 90,
                                min_replicas= 100,
                                max_replicas= 300,
                                scale_up_stabilization_window_seconds = 0,
                                scale_up_policy = scale_policy(
                                    policy_type = "Percent",
                                    value = 100,
                                    period_seconds = 60,
                                ),
                                scale_down_stabilization_window_seconds = 300,
                                scale_down_policy = scale_policy(
                                    policy_type = "Pods", # scale down by at most 40 pods every period
                                    value = 40,
                                    period_seconds = 1800, # scale down at most every 30 minutes
                                ),
                                enable_immediate_scaling = True,
                            ),
                        },
                        "bom": {
                            "consume": autoscaling_config(
                                host_set = "sigma",
                                service = "workers",
                                cluster = "bom",
                                availability_threshold = "0.60",
                                cpu_threshold = 80,
                                memory_threshold = 90,
                                min_replicas= 10,
                                max_replicas= 30,
                                scale_up_stabilization_window_seconds = 0,
                                scale_up_policy = scale_policy(
                                    policy_type = "Percent",
                                    value = 100,
                                    period_seconds = 60,
                                ),
                                scale_down_stabilization_window_seconds = 300,
                                scale_down_policy = scale_policy(
                                    policy_type = "Pods", # scale down by at most 4 pods every period
                                    value = 4,
                                    period_seconds = 1800, # scale down at most every 30 minutes
                                ),
                                enable_immediate_scaling = True,
                            ),
                        },
                    }
                }
            ),
        hostset("sigma-staging",
                default=            cluster(mb_per_worker=2500),
                qa=all_regions_empty_worker_cluster(),
                prod={
                    "cmh":          cluster(consume_replicas=0,
                                            fanout_replicas=0),
                    "northwest":    cluster(consume_replicas=2,
                                            fanout_replicas=2,
                                            mb_per_worker=2900),
                    "bom":          cluster(consume_replicas=0,
                                            fanout_replicas=0),

                }),
    ]
)

TEST_HOST_SET = struct(
    availability_tier=AVAILABILITY_TIER_A400,
    hostsets=[
        hostset("monster-test",
                prod={
                    "northwest":    cluster(consume_replicas=2, fanout_replicas=2, mb_per_worker=2800, instance_type=r6g_4xlarge),
                    "bom":          cluster(mb_per_worker=2800),
                    "cmh":          cluster(consume_replicas=2, fanout_replicas=2, mb_per_worker=2800, instance_type=r6g_4xlarge),
                },
                qa={
                    "northwest":    cluster(consume_replicas=2,
                                            fanout_replicas=2),
                    "bom":          cluster(consume_replicas=2,
                                            fanout_replicas=2),
                    "cmh":          cluster(consume_replicas=2,
                                            fanout_replicas=2),
                }),
    ]
)

def get_host_set_map_from_tier(tier):
  return [(hostset.name, struct(hostset=hostset, availability_tier=tier.availability_tier)) for hostset in tier.hostsets]

def get_entries_from_host_set_list(host_set_list):
    entries = []
    for host_set in host_set_list:
        entries.extend(get_host_set_map_from_tier(host_set))
    return entries

def generate_all_host_set_map(entries):
    output = {}
    for entry in entries:
        key, value = entry
        if output.get(key) == None:
            output.update({key: [value]})
        else:
            output.get(key).append(value)
    return output


HOST_SETS_LIST = [
    A100_HOST_SETS,
    A200_HOST_SETS,
    A400_HOST_SETS,
    SIGMA_HOST_SET,
    TEST_HOST_SET
]

ALL_HOST_SETS_ENTRIES = get_entries_from_host_set_list(HOST_SETS_LIST)

# This map allows looking up host set definitions by name: [host_set -> [hostset_structs]]
# If there are multiple "hostset" structs defined for a host set name (e.g. canary), it will include both in a unordered list
ALL_HOST_SETS_MAP = generate_all_host_set_map(ALL_HOST_SETS_ENTRIES)
