{
    "default": {
        "table_properties": {
            "source": "chronon"
        },
        "common_env": {
            "SPARK_SUBMIT_PATH": "[TODO]/path/to/spark-submit",
            "JOB_MODE": "local[*]",
            "HADOOP_DIR": "[STREAMING-TODO]/path/to/folder/containing",
            "CHRONON_ONLINE_CLASS": "[ONLINE-TODO]your.online.class",
            "CHRONON_ONLINE_ARGS": "[ONLINE-TODO]args prefixed with -Z become constructor map for your implementation of ai.chronon.online.Api, -Zkv-host=<YOUR_HOST> -Zkv-port=<YOUR_PORT>"
        },
        "production": {            
            "backfill" : {
                "EXECUTOR_CORES": "1",
                "DRIVER_MEMORY": "15G",
                "EXECUTOR_MEMORY": "8G",
                "PARALLELISM": "4000",
                "MAX_EXECUTORS": "1000"
            },
            "upload" : {
                "EXECUTOR_CORES": "1",
                "EXECUTOR_MEMORY": "8G",
                "PARALLELISM": "1000",
                "MAX_EXECUTORS": "1000"
            },
            "streaming" : {
                "EXECUTOR_CORES": "2",
                "EXECUTOR_MEMORY": "4G",
                "PARALLELISM": "16"
            }
        }
    },
    "sample_team": {
        "description": "Team description",
        "namespace": "# TODO: output namespace of hive tables that chronon produces",
        "user": "# TODO: ldap user name to run the jobs as, from airflow or your own scheduler",
        "production": {
            "backfill" : {
                "EXECUTOR_CORES": "4"
            }
        }
    }
}
