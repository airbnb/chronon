{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc94dc85-c506-4287-b809-b9133809dc8c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Test Suite\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Python testing suite\n",
    "\"\"\"\n",
    "from ai.chronon.repo.validator import ChrononRepoValidator\n",
    "from ai.chronon.repo.compile import _write_obj\n",
    "from ai.chronon.utils import get_underlying_source, get_query\n",
    "from ai.chronon.api import ttypes\n",
    "\n",
    "from ai.chronon.repo.run import Runner, download_jar\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import getpass\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Global vars.\n",
    "user = getpass.getuser()\n",
    "home = f\"/home/{user}/notebooks_home\"\n",
    "chronon_root = f\"{home}/repo/chronon\"\n",
    "output_folder = \"production\"\n",
    "output_root = os.path.join(chronon_root, output_folder)\n",
    "\n",
    "# Config specific vars\n",
    "team = \"chronon_test\"\n",
    "conf_name = f\"{user}__join_test.v1\"\n",
    "metadata_name = \".\".join([team, conf_name])\n",
    "conf = os.path.join(output_root, \"joins\", team, conf_name)\n",
    "\n",
    "\n",
    "# Add chronon modules to the path.\n",
    "sys.path.append(chronon_root)\n",
    "\n",
    "\n",
    "def mutate_obj(obj, modulo):\n",
    "    \"\"\"\n",
    "    Add additional where clauses for obj to reduce the input size.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, ttypes.Join):\n",
    "        mutate_join(obj, modulo)\n",
    "    elif isinstance(obj, ttypes.GroupBy):\n",
    "        mutate_group_by(obj, modulo)\n",
    "    return obj\n",
    "\n",
    "\n",
    "def additional_wheres(keys, query, modulo):\n",
    "    mapped_keys = map(lambda x: query.selects[x], keys) if query.selects is not None else keys\n",
    "    return [f\"hash({key}) % {modulo} = 0\" for key in mapped_keys]\n",
    "\n",
    "\n",
    "def mutate_group_by(obj, modulo):\n",
    "    \"\"\"\n",
    "    Group by modifications.\n",
    "    \"\"\"\n",
    "    keys = obj.keyColumns\n",
    "    for source in map(lambda x: get_underlying_source(x), obj.sources):\n",
    "        wheres = additional_wheres(keys, source.query, modulo)\n",
    "        source.query.wheres = wheres if not source.query.wheres else list(set(source.query.wheres + wheres))\n",
    "    return obj\n",
    "\n",
    "\n",
    "def mutate_join(obj, modulo):\n",
    "    \"\"\"\n",
    "    Mutate a join source to filter keys.\n",
    "    \"\"\"\n",
    "    keys = []\n",
    "    for jp in obj.joinParts:\n",
    "        keys.extend(jp.groupBy.keyColumns)\n",
    "    source = get_underlying_source(obj.left)\n",
    "    wheres = additional_wheres(keys, source.query, modulo)\n",
    "    source.query.wheres = wheres if not source.query.wheres else list(set(source.query.wheres + wheres))\n",
    "    for jp in obj.joinParts:\n",
    "        jp.groupBy = mutate_group_by(jp.groupBy, modulo)\n",
    "    return obj\n",
    "    \n",
    "\n",
    "class BaseArgs(object):\n",
    "    def __init__(self, obj, days = 10):\n",
    "        self.mode = 'backfill'\n",
    "        self.ds = (datetime.strptime(get_query(obj.left).startPartition, '%Y-%m-%d') + timedelta(days=days)).strftime('%Y-%m-%d')\n",
    "        self.conf = conf\n",
    "        self.repo = chronon_root\n",
    "        self.sub_help = False\n",
    "        self.online_jar = None\n",
    "        self.online_class = None\n",
    "        self.args = ''\n",
    "        self.app_name = obj.metaData.name.replace('.', '_')\n",
    "        self.spark_submit_path = os.path.join(chronon_root, 'scripts/spark_submit.sh')\n",
    "        self.list_apps = None\n",
    "                                                                                       \n",
    "def write_test_config(obj, modulo = 256, verbose = False):\n",
    "    \"\"\"\n",
    "    Write a temporary config for testing\n",
    "    \"\"\"\n",
    "    obj.metaData.name = metadata_name\n",
    "    obj.metaData.team = team\n",
    "    obj.metaData.outputNamespace = 'tmp'\n",
    "    mutate_obj(obj, modulo)\n",
    "    if _write_obj(\n",
    "        output_root, \n",
    "        validator=ChrononRepoValidator(\n",
    "            chronon_root_path=chronon_root, \n",
    "            output_root=output_folder), \n",
    "        name=metadata_name, \n",
    "        obj=obj, \n",
    "        log_level=None,\n",
    "        force_compile=False,\n",
    "        force_overwrite=True\n",
    "    ) and verbose:\n",
    "        with open(conf, 'r') as infile:\n",
    "            print(infile.read())\n",
    "\n",
    "\n",
    "def run_test_config(obj, days):\n",
    "    jar_path = download_jar(None)\n",
    "    os.environ[\"USER\"] = user\n",
    "    Runner(BaseArgs(obj, days= days), jar_path).run()\n",
    "print(\"Loaded Test Suite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3bc20d8-45e5-4851-8ba9-3a4d292061e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/default_user/.conda/envs/user/lib/python3.7/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Looking in indexes: https://artifactory.d.musta.ch/artifactory/api/pypi/pypi/simple, https://sssp-bighead-artifacts.d.musta.ch/release/wheels/, https://sssp-bighead-artifacts.d.musta.ch/branch/wheels/\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: chronon-ai in /home/USER/.conda/envs/user/lib/python3.7/site-packages (0.0.6)\n",
      "Requirement already satisfied: click in /home/USER/.conda/envs/user/lib/python3.7/site-packages (from chronon-ai) (8.1.3)\n",
      "Requirement already satisfied: thrift<0.14 in /home/USER/.conda/envs/user/lib/python3.7/site-packages (from chronon-ai) (0.11.0)\n",
      "Requirement already satisfied: six>=1.7.2 in /home/USER/.conda/envs/user/lib/python3.7/site-packages (from thrift<0.14->chronon-ai) (1.16.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/USER/.conda/envs/user/lib/python3.7/site-packages (from click->chronon-ai) (4.11.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/USER/.conda/envs/user/lib/python3.7/site-packages (from importlib-metadata->click->chronon-ai) (4.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/USER/.conda/envs/user/lib/python3.7/site-packages (from importlib-metadata->click->chronon-ai) (3.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install chronon-ai -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e0df659-8015-461d-9fc5-00cf2195b70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/USER/notebooks_home/repo/chronon/group_bys/chronon_test/test_tmp.py\n"
     ]
    }
   ],
   "source": [
    "%%file ~/notebooks_home/repo/chronon/group_bys/chronon_test/test_tmp.py\n",
    "\"\"\"\n",
    "Sample GroupBy for Join\n",
    "\"\"\"\n",
    "from airbnb import test_sources\n",
    "from ai.chronon.group_by import (\n",
    "    Aggregation,\n",
    "    GroupBy,\n",
    "    Operation,\n",
    "    TimeUnit,\n",
    "    Window,\n",
    "    Accuracy,\n",
    ")\n",
    "\n",
    "\n",
    "v2 = GroupBy(\n",
    "    sources=test_sources.listing__<metric>,\n",
    "    keys=[\"listing\"],\n",
    "    aggregations=[\n",
    "        Aggregation(input_column=\"m_guests\", operation=Operation.SUM, windows=[Window(7, TimeUnit.DAYS)]),\n",
    "        Aggregation(input_column=\"m_dated_<metric>\", operation=Operation.SUM),\n",
    "    ],\n",
    "    backfill_start_date=\"2022-01-14\",\n",
    "    output_namespace=\"chronon_test\",\n",
    "    dependencies=[\"<tablename>/ds={{ ds }}\"],\n",
    "    table_properties={\n",
    "        'abb_retention_config_json': '{\"policy\": \"delete_by_last_modified\", \"days\": 30}',\n",
    "    },\n",
    "    online=True,\n",
    "    production=True,\n",
    "    team_override='ml_infra',\n",
    "    accuracy=Accuracy.SNAPSHOT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "964b713f-2576-4789-a21f-281903a44965",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample Online Join on a small dataset.\n",
    "\"\"\"\n",
    "from airbnb import test_sources\n",
    "from ai.chronon.join import Join, JoinPart\n",
    "from group_bys.chronon_test import test_online_group_by_small, test_tmp\n",
    "from airbnb.data_sources_2 import HiveEventSource\n",
    "from ai.chronon.query import Query, select\n",
    "\n",
    "\n",
    "v1 = Join(\n",
    "    left=HiveEventSource(\n",
    "    namespace='global',\n",
    "    table=\"<tablename>\",\n",
    "    query=Query(\n",
    "        selects=select(\n",
    "            listing=\"id_product\",\n",
    "            m_guests=\"m_guests\",\n",
    "            m_dated_<metric>=\"m_dated_<metric>\"\n",
    "        ),\n",
    "        wheres=[\"<dimension> = 'VALUE'\"],\n",
    "        time_column=\"UNIX_TIMESTAMP(ts) * 1000\",\n",
    "        start_partition=\"2022-01-14\",\n",
    "        end_partition=\"2022-03-14\",\n",
    "    )),\n",
    "    right_parts=[\n",
    "        JoinPart(group_by=test_online_group_by_small.v1),\n",
    "        JoinPart(group_by=test_tmp.v2),\n",
    "    ],\n",
    "    online=True,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22b598c9-f338-4e27-81b6-5249428d1da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Join Team - \u001b[34mchronon_test\u001b[0m\n",
      "                Join Name - \u001b[34mUSER_polynote_join_test.v1\u001b[0m\n",
      "\u001b[33mForce overwrite Join USER_polynote_join_test.v1\u001b[0m\n",
      "          Writing Join to - \u001b[34m/home/USER/notebooks_home/repo/chronon/production/joins/chronon_test/USER_polynote_join_test.v1\u001b[0m\n",
      "{\n",
      "  \"metaData\": {\n",
      "    \"name\": \"chronon_test.USER_polynote_join_test.v1\",\n",
      "    \"online\": 1,\n",
      "    \"production\": 0,\n",
      "    \"customJson\": \"{\\\"check_consistency\\\": false, \\\"lag\\\": 0}\",\n",
      "    \"dependencies\": [\n",
      "      \"{\\\"name\\\": \\\"wait_for_<tablename>_ds\\\", \\\"spec\\\": \\\"<tablename>/ds={{ ds }}\\\", \\\"start\\\": \\\"2022-01-14\\\", \\\"end\\\": \\\"2022-03-14\\\"}\",\n",
      "      \"{\\\"name\\\": \\\"wait_for_<tablename>_ds\\\", \\\"spec\\\": \\\"<tablename>/ds={{ ds }}\\\", \\\"start\\\": \\\"2022-01-14\\\", \\\"end\\\": null}\"\n",
      "    ],\n",
      "    \"outputNamespace\": \"tmp\",\n",
      "    \"team\": \"chronon_test\"\n",
      "  },\n",
      "  \"left\": {\n",
      "    \"events\": {\n",
      "      \"table\": \"<tablename>\",\n",
      "      \"query\": {\n",
      "        \"selects\": {\n",
      "          \"listing\": \"id_product\",\n",
      "          \"m_guests\": \"m_guests\",\n",
      "          \"m_dated_<metric>\": \"m_dated_<metric>\",\n",
      "          \"ts\": \"UNIX_TIMESTAMP(ts) * 1000\"\n",
      "        },\n",
      "        \"wheres\": [\n",
      "          \"hash(id_product) % 256 = 0\",\n",
      "          \"<dimension> = 'VALUE'\"\n",
      "        ],\n",
      "        \"startPartition\": \"2022-01-14\",\n",
      "        \"endPartition\": \"2022-03-14\",\n",
      "        \"timeColumn\": \"UNIX_TIMESTAMP(ts) * 1000\",\n",
      "        \"setups\": []\n",
      "      },\n",
      "      \"isCumulative\": 0\n",
      "    }\n",
      "  },\n",
      "  \"joinParts\": [\n",
      "    {\n",
      "      \"groupBy\": {\n",
      "        \"metaData\": {\n",
      "          \"name\": \"chronon_test.test_online_group_by_small.v1\",\n",
      "          \"online\": 1,\n",
      "          \"production\": 1,\n",
      "          \"customJson\": \"{\\\"team_override\\\": \\\"ml_infra\\\", \\\"lag\\\": 0}\",\n",
      "          \"dependencies\": [\n",
      "            \"{\\\"name\\\": \\\"wait_for_global.pdp_fct_views_global_pdp_fct_views_ds_ds\\\", \\\"spec\\\": \\\"<tablename>/ds={{ ds }}\\\", \\\"start\\\": \\\"2022-01-14\\\", \\\"end\\\": null}\"\n",
      "          ],\n",
      "          \"tableProperties\": {\n",
      "            \"abb_retention_config_json\": \"{\\\"policy\\\": \\\"delete_by_last_modified\\\", \\\"days\\\": 30}\"\n",
      "          },\n",
      "          \"outputNamespace\": \"chronon_test\",\n",
      "          \"team\": \"chronon_test\"\n",
      "        },\n",
      "        \"sources\": [\n",
      "          {\n",
      "            \"events\": {\n",
      "              \"table\": \"<tablename>\",\n",
      "              \"query\": {\n",
      "                \"selects\": {\n",
      "                  \"listing\": \"id_product\",\n",
      "                  \"m_guests\": \"m_guests\",\n",
      "                  \"m_dated_<metric>\": \"m_dated_<metric>\"\n",
      "                },\n",
      "                \"wheres\": [\n",
      "                  \"hash(id_product) % 256 = 0\",\n",
      "                  \"<dimension> = 'VALUE'\"\n",
      "                ],\n",
      "                \"startPartition\": \"2022-01-14\",\n",
      "                \"timeColumn\": \"UNIX_TIMESTAMP(ts) * 1000\",\n",
      "                \"setups\": []\n",
      "              },\n",
      "              \"isCumulative\": 0\n",
      "            }\n",
      "          }\n",
      "        ],\n",
      "        \"keyColumns\": [\n",
      "          \"listing\"\n",
      "        ],\n",
      "        \"aggregations\": [\n",
      "          {\n",
      "            \"inputColumn\": \"m_guests\",\n",
      "            \"operation\": 7,\n",
      "            \"argMap\": {},\n",
      "            \"windows\": [\n",
      "              {\n",
      "                \"length\": 7,\n",
      "                \"timeUnit\": 1\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"inputColumn\": \"m_dated_<metric>\",\n",
      "            \"operation\": 7,\n",
      "            \"argMap\": {}\n",
      "          }\n",
      "        ],\n",
      "        \"accuracy\": 1,\n",
      "        \"backfillStartDate\": \"2022-01-14\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"groupBy\": {\n",
      "        \"metaData\": {\n",
      "          \"name\": \"chronon_test.test_tmp.v2\",\n",
      "          \"online\": 1,\n",
      "          \"production\": 1,\n",
      "          \"customJson\": \"{\\\"team_override\\\": \\\"ml_infra\\\", \\\"lag\\\": 0}\",\n",
      "          \"dependencies\": [\n",
      "            \"{\\\"name\\\": \\\"wait_for_global.pdp_fct_views_global_pdp_fct_views_ds_ds\\\", \\\"spec\\\": \\\"<tablename>/ds={{ ds }}\\\", \\\"start\\\": \\\"2022-01-14\\\", \\\"end\\\": null}\"\n",
      "          ],\n",
      "          \"tableProperties\": {\n",
      "            \"abb_retention_config_json\": \"{\\\"policy\\\": \\\"delete_by_last_modified\\\", \\\"days\\\": 30}\"\n",
      "          },\n",
      "          \"outputNamespace\": \"chronon_test\",\n",
      "          \"team\": \"chronon_test\"\n",
      "        },\n",
      "        \"sources\": [\n",
      "          {\n",
      "            \"events\": {\n",
      "              \"table\": \"<tablename>\",\n",
      "              \"query\": {\n",
      "                \"selects\": {\n",
      "                  \"listing\": \"id_product\",\n",
      "                  \"m_guests\": \"m_guests\",\n",
      "                  \"m_dated_<metric>\": \"m_dated_<metric>\"\n",
      "                },\n",
      "                \"wheres\": [\n",
      "                  \"hash(id_product) % 256 = 0\",\n",
      "                  \"<dimension> = 'VALUE'\"\n",
      "                ],\n",
      "                \"startPartition\": \"2022-01-14\",\n",
      "                \"timeColumn\": \"UNIX_TIMESTAMP(ts) * 1000\",\n",
      "                \"setups\": []\n",
      "              },\n",
      "              \"isCumulative\": 0\n",
      "            }\n",
      "          }\n",
      "        ],\n",
      "        \"keyColumns\": [\n",
      "          \"listing\"\n",
      "        ],\n",
      "        \"aggregations\": [\n",
      "          {\n",
      "            \"inputColumn\": \"m_guests\",\n",
      "            \"operation\": 7,\n",
      "            \"argMap\": {},\n",
      "            \"windows\": [\n",
      "              {\n",
      "                \"length\": 7,\n",
      "                \"timeUnit\": 1\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"inputColumn\": \"m_dated_<metric>\",\n",
      "            \"operation\": 7,\n",
      "            \"argMap\": {}\n",
      "          }\n",
      "        ],\n",
      "        \"accuracy\": 1,\n",
      "        \"backfillStartDate\": \"2022-01-14\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "write_test_config(v1, modulo = 256, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c266b0b2-64b4-4522-a682-c3c7d11d0df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: curl -s https://s01.oss.sonatype.org/service/local/repositories/public/content/ai/chronon/spark_uber_2.11/maven-metadata.xml\n",
      "Running command: curl -sI https://s01.oss.sonatype.org/service/local/repositories/public/content/ai/chronon/spark_uber_2.11/0.0.7/spark_uber_2.11-0.0.7-assembly.jar\n",
      "Running command: wc -c /tmp/spark_uber_2.11-0.0.7-assembly.jar\n",
      "Files sizes of https://s01.oss.sonatype.org/service/local/repositories/public/content/ai/chronon/spark_uber_2.11/0.0.7/spark_uber_2.11-0.0.7-assembly.jar vs. /tmp/spark_uber_2.11-0.0.7-assembly.jar\n",
      "    Remote size: 25456697\n",
      "    Local size : 25456697\n",
      "Sizes match. Assuming its already downloaded.\n",
      "Setting env variables:\n",
      "Found EMR_CLUSTER=<emr cluster>\n",
      "Found EMR_QUEUE=backfill\n",
      "Found EXECUTOR_CORES=1\n",
      "Found DRIVER_MEMORY=15G\n",
      "Found EXECUTOR_MEMORY=8G\n",
      "Found PARALLELISM=4000\n",
      "Found MAX_EXECUTORS=1000\n",
      "Found APP_NAME=chronon_test_USER_polynote_join_test_v0\n",
      "Found CHRONON_CONF_PATH=/home/USER/notebooks_home/repo/chronon/production/joins/chronon_test/USER_polynote_join_test.v0\n",
      "Found CHRONON_DRIVER_JAR=/tmp/spark_uber_2.11-0.0.7-assembly.jar\n",
      "Running command: bash /home/USER/notebooks_home/repo/chronon/scripts/spark_submit.sh --class ai.chronon.spark.Driver /tmp/spark_uber_2.11-0.0.7-assembly.jar join --conf-path=/home/USER/notebooks_home/repo/chronon/production/joins/chronon_test/USER_polynote_join_test.v1 --end-date=2022-01-28  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+ mkdir -p /tmp/USER\n",
      "+ export LOG4J_FILE=/tmp/USER/log4j_file\n",
      "+ LOG4J_FILE=/tmp/USER/log4j_file\n",
      "+ cat\n",
      "+ export TEST_NAME=chronon_test_USER_polynote_join_test_v0_USER_test\n",
      "+ TEST_NAME=chronon_test_USER_polynote_join_test_v0_USER_test\n",
      "+ unset PYSPARK_DRIVER_PYTHON\n",
      "+ unset PYSPARK_PYTHON\n",
      "+ unset SPARK_HOME\n",
      "+ unset SPARK_CONF_DIR\n",
      "+ grep -v YarnScheduler:70\n",
      "+ emr-spark-submit --spark-version 2.4.0 --emr-cluster <emr cluster> --hive-cluster silver --queue backfill --driver-java-options ' -Dlog4j.configuration=file:/tmp/USER/log4j_file' --conf 'spark.executor.extraJavaOptions= -XX:ParallelGCThreads=4 -XX:+UseParallelGC -XX:+UseCompressedOops' --conf spark.reducer.maxReqsInFlight=1024 --conf spark.reducer.maxBlocksInFlightPerAddress=1024 --conf spark.reducer.maxSizeInFlight=256M --conf spark.shuffle.file.buffer=1M --conf spark.shuffle.service.enabled=true --conf spark.shuffle.service.index.cache.entries=2048 --conf spark.shuffle.io.serverThreads=128 --conf spark.shuffle.io.backLog=1024 --conf spark.shuffle.registration.timeout=2m --conf spark.sql.broadcastTimeout=1200 --conf spark.shuffle.registration.maxAttempts=5 --conf spark.unsafe.sorter.spill.reader.buffer.size=1M --conf spark.sql.shuffle.partitions=4000 --conf spark.kryoserializer.buffer.max=2000 --conf spark.rdd.compress=true --conf spark.shuffle.compress=true --conf spark.shuffle.spill.compress=true --conf spark.io.compression.codec=zstd --conf spark.io.compression.zstd.level=2 --conf spark.io.compression.zstd.bufferSize=1M --conf spark.dynamicAllocation.enabled=true --conf spark.dynamicAllocation.minExecutors=2 --conf spark.dynamicAllocation.maxExecutors=1000 --conf spark.default.parallelism=4000 --conf spark.port.maxRetries=64 --conf spark.task.maxFailures=20 --conf spark.stage.maxConsecutiveAttempts=12 --conf spark.maxRemoteBlockSizeFetchToMem=2G --conf spark.network.timeout=230s --conf spark.executor.heartbeatInterval=200s --conf spark.local.dir=/tmp --conf spark.jars.ivy=/tmp --conf spark.executor.cores=1 --conf spark.sql.files.maxPartitionBytes=1073741824 --conf spark.debug.maxToStringFields=1000 --conf spark.driver.maxResultSize=32G --deploy-mode client --master yarn --executor-memory 8G --driver-memory 15G --conf spark.executor.memoryOverhead=2G --conf spark.app.name=chronon_test_USER_polynote_join_test_v0 --conf spark.airflow.dag.name=chronon_test_USER_polynote_join_test_v0_USER_test --conf spark.airflow.taskId=chronon_test_USER_polynote_join_test_v0_USER_test --conf spark.airflow.attemptId=1 --conf 'spark.airflow.cost.meta=\\{}' --jars '' --class ai.chronon.spark.Driver /tmp/spark_uber_2.11-0.0.7-assembly.jar join --conf-path=/home/USER/notebooks_home/repo/chronon/production/joins/chronon_test/USER_polynote_join_test.v1 --end-date=2022-01-28\n",
      "+ grep -v TransportResponseHandler:144\n",
      "+ grep -v TransportClient:331\n",
      "+ grep -v io.netty.channel.AbstractChannel\n",
      "+ grep -v ClosedChannelException\n",
      "+ grep -v TransportResponseHandler:154\n",
      "+ grep -v TransportRequestHandler:293\n",
      "+ grep -v TransportResponseHandler:144\n",
      "+ tee /tmp/chronon_test_USER_polynote_join_test_v0_spark.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Spark client dependencies...\n",
      "Namespace(caller='spark-submit-k8s', cluster_info_directory='s3://sssp/data-infra/emr-client-config/', data_infra_git_sha='', dev_mode=False, emr_cluster='<emr cluster>', host_env='k8s', metastore='silver', name='spark', skip_update=False, version='2.4.0')\n",
      "Running command: aws s3 cp s3://sssp/data-infra/emr-client-config/cluster-info.yaml /tmp/cluster-info-0d3b3fe1-65a8-4059-8271-c4066f4ff7d8.yaml\n",
      "================================= Resolving dependency: spark version: 2.4.0 =================================\n",
      "Running command: aws s3api head-object --bucket sssp --key hadoop-emr/libs/emr-5.30.2-0.tar.gz\n",
      "Artifact has not changed, skipping downloading.\n",
      "prepare_dwi_client_deps_kvp:: env_var: SPARK_HOME, value: /mnt/dwi_client/spark/2.4.0/current/spark_2.4.0_uncompressed/emr-5.30.2-0/spark\n",
      "================================= Resolving dependency: spark version: 2.4.* =================================\n",
      "The link /mnt/dwi_client/spark/2.4.0/current/spark_2.4.0_uncompressed/emr-5.30.2-0/ to /mnt/opt/hadoop-emr/emr-current already exists, skipping\n",
      "=========================== Resolving dependency: spark-hive-libs version: 3a37fbc ===========================\n",
      "Running command: aws s3api head-object --bucket sssp --key spark/hive_jars/hive-2.3.4_jars_3a37fbc.tar.gz\n",
      "Artifact has not changed, skipping downloading.\n",
      "The link /mnt/dwi_client/spark-hive-libs/3a37fbc/current/spark-hive-libs_3a37fbc_uncompressed/next to /srv/emr-spark/hive_jars/3a37fbc already exists, skipping\n",
      "================================= Resolving dependency: s3a version: default =================================\n",
      "Running command: aws s3api head-object --bucket sssp --key data-infra/s3fs/airbnb-s3aplus-all-1.0.4.jar\n",
      "Artifact has not changed, skipping downloading.\n",
      "The link /mnt/dwi_client/s3a/default/current/airbnb-s3aplus-all-1.0.4.jar to /srv/emr-spark/hive_jars/3a37fbc/airbnb-s3aplus.jar already exists, skipping\n",
      "The link /mnt/dwi_client/s3a/default/current/airbnb-s3aplus-all-1.0.4.jar to /srv/hadoop/lib_versionless/airbnb-s3aplus.jar already exists, skipping\n",
      "========================= Resolving dependency: spark-listeners-all version: default =========================\n",
      "Running command: aws s3api head-object --bucket sssp --key data-infra/dwi_client/grouped_artifacts/spark_listeners.tar.gz\n",
      "Artifact has not changed, skipping downloading.\n",
      "======================== Resolving dependency: spark-atlas-connector version: default ========================\n",
      "The link /mnt/dwi_client/spark-listeners-all/default/current/spark-listeners-all_default_uncompressed/spark-atlas-connector-latest.jar to /mnt/hadoop-emr/extensions/spark/2.4/listeners/spark-atlas-connector-assembly-0.1.0-SNAPSHOT.jar already exists, skipping\n",
      "=========================== Resolving dependency: spark-listeners version: default ===========================\n",
      "The link /mnt/dwi_client/spark-listeners-all/default/current/spark-listeners-all_default_uncompressed/spark-listeners-24-latest.jar to /mnt/hadoop-emr/extensions/spark/2.4/listeners/spark-listeners/current already exists, skipping\n",
      "======================= Resolving dependency: spark-listeners-loader version: default ========================\n",
      "The link /mnt/dwi_client/spark-listeners-all/default/current/spark-listeners-all_default_uncompressed/spark-listeners-loader-latest.jar to /mnt/hadoop-emr/extensions/spark/2.4/common/listeners/spark-listeners-loader/current already exists, skipping\n",
      "======================= Resolving dependency: spark-listeners-common version: default ========================\n",
      "The link /mnt/dwi_client/spark-listeners-all/default/current/spark-listeners-all_default_uncompressed/spark-listeners-common-latest.jar to /mnt/hadoop-emr/extensions/spark/2.4/common/listeners/spark-listeners-common/current already exists, skipping\n",
      "============================= Resolving dependency: icehouse-spark version: 2.* ==============================\n",
      "Running command: aws s3api head-object --bucket sssp --key data-infra/icehouse/spark2/icehouse-spark2-current.jar\n",
      "Artifact has not changed, skipping downloading.\n",
      "The link /mnt/dwi_client/icehouse-spark/2.*/current/icehouse-spark2-current.jar to /mnt/hadoop-emr/extensions/spark/2/icehouse/current already exists, skipping\n",
      "======================= Resolving dependency: emr-client-config-spark version: 2.3.4.1 =======================\n",
      "===================== Resolving dependency: emr-client-config-spark-all version: default =====================\n",
      "Running command: aws s3api head-object --bucket sssp --key data-infra/emr-client-config/spark/<emr cluster>.tar.gz\n",
      "Artifact has not changed, skipping downloading.\n",
      "prepare_dwi_client_deps_kvp:: env_var: SPARK_CONF_DIR, value: /mnt/dwi_client/emr-client-config-spark-all/default/<emr cluster>/current/emr-client-config-spark-all_default_uncompressed/silver/spark/2.3.4.1\n",
      "prepare_dwi_client_deps_kvp:: env_var: HADOOP_CONF_DIR, value: /mnt/dwi_client/emr-client-config-spark-all/default/<emr cluster>/current/emr-client-config-spark-all_default_uncompressed/silver/spark/2.3.4.1\n",
      "2022-09-06 21:24:47 WARN  SparkConf:66 - The configuration key 'spark.shuffle.service.index.cache.entries' has been deprecated as of Spark 2.3.0 and may be removed in the future. Not used anymore. Please use spark.shuffle.service.index.cache.size\n",
      "2022-09-06 21:24:48 WARN  SparkConf:66 - The configuration key 'spark.shuffle.service.index.cache.entries' has been deprecated as of Spark 2.3.0 and may be removed in the future. Not used anymore. Please use spark.shuffle.service.index.cache.size\n",
      "2022-09-06 21:24:48 INFO  SparkContext:54 - Running Spark version 2.4.0\n",
      "2022-09-06 21:24:48 WARN  SparkConf:66 - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "2022-09-06 21:24:48 INFO  SparkContext:54 - Submitted application: join_chronon_test.USER_polynote_join_test.v1\n",
      "2022-09-06 21:24:48 INFO  SecurityManager:54 - Changing view acls to: USER\n",
      "2022-09-06 21:24:48 INFO  SecurityManager:54 - Changing modify acls to: USER\n",
      "2022-09-06 21:24:48 INFO  SecurityManager:54 - Changing view acls groups to: \n",
      "2022-09-06 21:24:48 INFO  SecurityManager:54 - Changing modify acls groups to: \n",
      "2022-09-06 21:24:48 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(USER); groups with view permissions: Set(); users  with modify permissions: Set(USER); groups with modify permissions: Set()\n",
      "2022-09-06 21:24:49 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 33531.\n",
      "2022-09-06 21:24:49 INFO  SparkEnv:54 - Registering MapOutputTracker\n",
      "2022-09-06 21:24:49 INFO  SparkEnv:54 - Registering BlockManagerMaster\n",
      "2022-09-06 21:24:49 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2022-09-06 21:24:49 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up\n",
      "2022-09-06 21:24:49 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-1aaa2438-11b1-4de2-b6df-022e2fc9c32c\n",
      "2022-09-06 21:24:49 INFO  MemoryStore:54 - MemoryStore started with capacity 7.8 GB\n",
      "2022-09-06 21:24:49 INFO  SparkEnv:54 - Registering OutputCommitCoordinator\n",
      "2022-09-06 21:24:49 INFO  log:192 - Logging initialized @2945ms\n",
      "2022-09-06 21:24:49 INFO  Server:351 - jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
      "2022-09-06 21:24:49 INFO  Server:419 - Started @3020ms\n",
      "2022-09-06 21:24:49 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2022-09-06 21:24:49 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "2022-09-06 21:24:49 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "2022-09-06 21:24:49 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "2022-09-06 21:24:49 INFO  AbstractConnector:278 - Started ServerConnector@431f1eaf{HTTP/1.1,[http/1.1]}{0.0.0.0:4044}\n",
      "2022-09-06 21:24:49 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4044.\n",
      "2022-09-06 21:24:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@11d4dbd6{/jobs,null,AVAILABLE,@Spark}\n",
      "2022-09-06 21:24:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2cec704c{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2022-09-06 21:24:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@771cbb1a{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2022-09-06 21:24:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6b2e0f78{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2022-09-06 21:24:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@240f6c41{/stages,null,AVAILABLE,@Spark}\n",
      "2022-09-06 21:24:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3659d7b1{/stages/json,null,AVAILABLE,@Spark}\n",
      "2022-09-06 21:24:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2015b2cd{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2022-09-06 21:24:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1c758545{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2022-09-06 21:24:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@117bcfdc{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2022-09-06 21:24:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@73a19967{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2022-09-06 21:24:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5e746d37{/storage,null,AVAILABLE,@Spark}\n",
      "2022-09-06 21:24:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6e1b9411{/storage/json,null,AVAILABLE,@Spark}\n",
      "2022-09-06 21:24:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@21d1b321{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2022-09-06 21:24:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5ec46cdd{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2022-09-06 21:24:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2324bfe7{/environment,null,AVAILABLE,@Spark}\n",
      "2022-09-06 21:24:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@112d1c8e{/environment/json,null,AVAILABLE,@Spark}\n",
      "2022-09-06 21:24:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3d49fd31{/executors,null,AVAILABLE,@Spark}\n",
      "2022-09-06 21:24:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4016ccc1{/executors/json,null,AVAILABLE,@Spark}\n",
      "2022-09-06 21:24:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@46cb98a3{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2022-09-06 21:24:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3ffb3598{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2022-09-06 21:24:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4da9f723{/static,null,AVAILABLE,@Spark}\n",
      "2022-09-06 21:24:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@73545b80{/,null,AVAILABLE,@Spark}\n",
      "2022-09-06 21:24:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6d469831{/api,null,AVAILABLE,@Spark}\n",
      "2022-09-06 21:24:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2776015d{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2022-09-06 21:24:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@b174a73{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2022-09-06 21:24:49 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://100.117.121.240:4044\n",
      "2022-09-06 21:24:49 INFO  SparkContext:54 - Added JAR file:/tmp/spark_uber_2.11-0.0.7-assembly.jar at spark://100.117.121.240:33531/jars/spark_uber_2.11-0.0.7-assembly.jar with timestamp 1662499489428\n",
      "2022-09-06 21:24:49 WARN  Utils:66 - spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n",
      "2022-09-06 21:24:49 INFO  Utils:54 - Using initial executors = 2, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "2022-09-06 21:24:51 INFO  Client:938 - Retrying connect to server: ip-172-21-250-56.ec2.internal/172.21.250.56:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=4, sleepTime=1000 MILLISECONDS)\n",
      "2022-09-06 21:24:52 INFO  Client:938 - Retrying connect to server: ip-172-21-250-56.ec2.internal/172.21.250.56:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=4, sleepTime=1000 MILLISECONDS)\n",
      "2022-09-06 21:24:53 INFO  Client:938 - Retrying connect to server: ip-172-21-250-56.ec2.internal/172.21.250.56:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=4, sleepTime=1000 MILLISECONDS)\n",
      "2022-09-06 21:24:54 INFO  Client:938 - Retrying connect to server: ip-172-21-250-56.ec2.internal/172.21.250.56:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=4, sleepTime=1000 MILLISECONDS)\n",
      "2022-09-06 21:24:54 WARN  Client:920 - Failed to connect to server: ip-172-21-250-56.ec2.internal/172.21.250.56:8032: retries get failed due to exceeded maximum allowed retries number: 4\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)\n",
      "\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n",
      "\tat com.sun.proxy.$Proxy17.getClusterMetrics(Unknown Source)\n",
      "\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getClusterMetrics(ApplicationClientProtocolPBClientImpl.java:243)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n",
      "\tat com.sun.proxy.$Proxy18.getClusterMetrics(Unknown Source)\n",
      "\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getYarnClusterMetrics(YarnClientImpl.java:550)\n",
      "\tat org.apache.spark.deploy.yarn.Client$$anonfun$submitApplication$1.apply(Client.scala:164)\n",
      "\tat org.apache.spark.deploy.yarn.Client$$anonfun$submitApplication$1.apply(Client.scala:164)\n",
      "\tat org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)\n",
      "\tat org.apache.spark.deploy.yarn.Client.logInfo(Client.scala:59)\n",
      "\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:163)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:178)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:501)\n",
      "\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2520)\n",
      "\tat org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:935)\n",
      "\tat org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:926)\n",
      "\tat scala.Option.getOrElse(Option.scala:121)\n",
      "\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:926)\n",
      "\tat ai.chronon.spark.SparkSessionBuilder$.build(SparkSessionBuilder.scala:51)\n",
      "\tat ai.chronon.spark.Driver$JoinBackfill$.run(Driver.scala:69)\n",
      "\tat ai.chronon.spark.Driver$.main(Driver.scala:380)\n",
      "\tat ai.chronon.spark.Driver.main(Driver.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:849)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "2022-09-06 21:24:54 INFO  ConfiguredRMFailoverProxyProvider:100 - Failing over to rm2\n",
      "2022-09-06 21:24:55 INFO  Client:938 - Retrying connect to server: ip-172-21-237-180.ec2.internal/172.21.237.180:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=4, sleepTime=1000 MILLISECONDS)\n",
      "2022-09-06 21:24:56 INFO  Client:938 - Retrying connect to server: ip-172-21-237-180.ec2.internal/172.21.237.180:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=4, sleepTime=1000 MILLISECONDS)\n",
      "2022-09-06 21:24:57 INFO  Client:938 - Retrying connect to server: ip-172-21-237-180.ec2.internal/172.21.237.180:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=4, sleepTime=1000 MILLISECONDS)\n",
      "2022-09-06 21:24:58 INFO  Client:938 - Retrying connect to server: ip-172-21-237-180.ec2.internal/172.21.237.180:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=4, sleepTime=1000 MILLISECONDS)\n",
      "2022-09-06 21:24:58 WARN  Client:920 - Failed to connect to server: ip-172-21-237-180.ec2.internal/172.21.237.180:8032: retries get failed due to exceeded maximum allowed retries number: 4\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)\n",
      "\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n",
      "\tat com.sun.proxy.$Proxy17.getClusterMetrics(Unknown Source)\n",
      "\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getClusterMetrics(ApplicationClientProtocolPBClientImpl.java:243)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n",
      "\tat com.sun.proxy.$Proxy18.getClusterMetrics(Unknown Source)\n",
      "\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getYarnClusterMetrics(YarnClientImpl.java:550)\n",
      "\tat org.apache.spark.deploy.yarn.Client$$anonfun$submitApplication$1.apply(Client.scala:164)\n",
      "\tat org.apache.spark.deploy.yarn.Client$$anonfun$submitApplication$1.apply(Client.scala:164)\n",
      "\tat org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)\n",
      "\tat org.apache.spark.deploy.yarn.Client.logInfo(Client.scala:59)\n",
      "\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:163)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:178)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:501)\n",
      "\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2520)\n",
      "\tat org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:935)\n",
      "\tat org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:926)\n",
      "\tat scala.Option.getOrElse(Option.scala:121)\n",
      "\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:926)\n",
      "\tat ai.chronon.spark.SparkSessionBuilder$.build(SparkSessionBuilder.scala:51)\n",
      "\tat ai.chronon.spark.Driver$JoinBackfill$.run(Driver.scala:69)\n",
      "\tat ai.chronon.spark.Driver$.main(Driver.scala:380)\n",
      "\tat ai.chronon.spark.Driver.main(Driver.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:849)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "2022-09-06 21:24:58 INFO  RetryInvocationHandler:398 - Exception while invoking ApplicationClientProtocolPBClientImpl.getClusterMetrics over rm2 after 1 failover attempts. Trying to failover after sleeping for 39155ms.\n",
      "java.net.ConnectException: Call From jupyter-cristian-5ffigueroa/100.117.121.240 to ip-172-21-237-180.ec2.internal:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)\n",
      "\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)\n",
      "\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1435)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n",
      "\tat com.sun.proxy.$Proxy17.getClusterMetrics(Unknown Source)\n",
      "\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getClusterMetrics(ApplicationClientProtocolPBClientImpl.java:243)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n",
      "\tat com.sun.proxy.$Proxy18.getClusterMetrics(Unknown Source)\n",
      "\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getYarnClusterMetrics(YarnClientImpl.java:550)\n",
      "\tat org.apache.spark.deploy.yarn.Client$$anonfun$submitApplication$1.apply(Client.scala:164)\n",
      "\tat org.apache.spark.deploy.yarn.Client$$anonfun$submitApplication$1.apply(Client.scala:164)\n",
      "\tat org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)\n",
      "\tat org.apache.spark.deploy.yarn.Client.logInfo(Client.scala:59)\n",
      "\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:163)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:178)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:501)\n",
      "\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2520)\n",
      "\tat org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:935)\n",
      "\tat org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:926)\n",
      "\tat scala.Option.getOrElse(Option.scala:121)\n",
      "\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:926)\n",
      "\tat ai.chronon.spark.SparkSessionBuilder$.build(SparkSessionBuilder.scala:51)\n",
      "\tat ai.chronon.spark.Driver$JoinBackfill$.run(Driver.scala:69)\n",
      "\tat ai.chronon.spark.Driver$.main(Driver.scala:380)\n",
      "\tat ai.chronon.spark.Driver.main(Driver.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:849)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)\n",
      "\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n",
      "\t... 45 more\n",
      "2022-09-06 21:25:37 INFO  ConfiguredRMFailoverProxyProvider:100 - Failing over to rm3\n",
      "2022-09-06 21:25:37 INFO  Client:54 - Requesting a new application from cluster with 120 NodeManagers\n",
      "2022-09-06 21:25:38 INFO  Client:54 - Verifying our application has not requested more than the maximum memory capability of the cluster (65536 MB per container)\n",
      "2022-09-06 21:25:38 INFO  Client:54 - Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "2022-09-06 21:25:38 INFO  Client:54 - Setting up container launch context for our AM\n",
      "2022-09-06 21:25:38 INFO  Client:54 - Setting up the launch environment for our AM container\n",
      "2022-09-06 21:25:38 INFO  Client:54 - Preparing resources for our AM container\n",
      "2022-09-06 21:25:38 WARN  Client:66 - Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "2022-09-06 21:25:39 INFO  Client:54 - Uploading resource file:/tmp/spark-b2ce4e8f-2079-431b-b3ab-32e2101309e0/__spark_libs__4656337071840458848.zip -> hdfs://ha-nn-uri/user/USER/.sparkStaging/application_1654538887658_324476/__spark_libs__4656337071840458848.zip\n",
      "2022-09-06 21:25:40 INFO  Client:54 - Uploading resource file:/tmp/spark-b2ce4e8f-2079-431b-b3ab-32e2101309e0/__spark_conf__7742303459264924741.zip -> hdfs://ha-nn-uri/user/USER/.sparkStaging/application_1654538887658_324476/__spark_conf__.zip\n",
      "2022-09-06 21:25:40 INFO  SecurityManager:54 - Changing view acls to: USER\n",
      "2022-09-06 21:25:40 INFO  SecurityManager:54 - Changing modify acls to: USER\n",
      "2022-09-06 21:25:40 INFO  SecurityManager:54 - Changing view acls groups to: \n",
      "2022-09-06 21:25:40 INFO  SecurityManager:54 - Changing modify acls groups to: \n",
      "2022-09-06 21:25:40 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(USER); groups with view permissions: Set(); users  with modify permissions: Set(USER); groups with modify permissions: Set()\n",
      "2022-09-06 21:25:41 INFO  Client:54 - Submitting application application_1654538887658_324476 to ResourceManager\n",
      "2022-09-06 21:25:41 INFO  YarnClientImpl:278 - Submitted application application_1654538887658_324476\n",
      "2022-09-06 21:25:41 INFO  SchedulerExtensionServices:54 - Starting Yarn extension services with app application_1654538887658_324476 and attemptId None\n",
      "2022-09-06 21:25:42 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:25:42 INFO  Client:54 - \n",
      "\t client token: N/A\n",
      "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: backfill\n",
      "\t start time: 1662499541443\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-21-250-56.ec2.internal:20888/proxy/application_1654538887658_324476/\n",
      "\t user: USER\n",
      "2022-09-06 21:25:43 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:25:44 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:25:45 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:25:46 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:25:47 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:25:48 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:25:49 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:25:50 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:25:51 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:25:52 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:25:53 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:25:54 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:25:55 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:25:56 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:25:57 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:25:58 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:25:59 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:00 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:01 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:02 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:03 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:04 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:05 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:06 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:07 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:08 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:09 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:10 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:11 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:12 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:13 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:14 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:15 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:16 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:17 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:18 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:19 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:20 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:21 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:22 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:23 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:24 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:25 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:26 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:27 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:28 INFO  Client:54 - Application report for application_1654538887658_324476 (state: ACCEPTED)\n",
      "2022-09-06 21:26:29 INFO  YarnClientSchedulerBackend:54 - Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> 172.21.250.56, PROXY_URI_BASES -> http://172.21.250.56:20888/proxy/application_1654538887658_324476, RM_HA_URLS -> 172.21.250.56:8088,172.21.237.180:8088,172.21.227.71:8088), /proxy/application_1654538887658_324476\n",
      "2022-09-06 21:26:29 INFO  JettyUtils:54 - Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.\n",
      "2022-09-06 21:26:29 INFO  Client:54 - Application report for application_1654538887658_324476 (state: RUNNING)\n",
      "2022-09-06 21:26:29 INFO  Client:54 - \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 172.21.229.167\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: backfill\n",
      "\t start time: 1662499541443\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-21-250-56.ec2.internal:20888/proxy/application_1654538887658_324476/\n",
      "\t user: USER\n",
      "2022-09-06 21:26:29 INFO  YarnClientSchedulerBackend:54 - Application application_1654538887658_324476 has started running.\n",
      "2022-09-06 21:26:29 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42261.\n",
      "2022-09-06 21:26:29 INFO  NettyBlockTransferService:54 - Server created on 100.117.121.240:42261\n",
      "2022-09-06 21:26:29 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2022-09-06 21:26:29 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 100.117.121.240, 42261, None)\n",
      "2022-09-06 21:26:29 INFO  YarnSchedulerBackend$YarnSchedulerEndpoint:54 - ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "2022-09-06 21:26:29 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 100.117.121.240:42261 with 7.8 GB RAM, BlockManagerId(driver, 100.117.121.240, 42261, None)\n",
      "2022-09-06 21:26:29 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 100.117.121.240, 42261, None)\n",
      "2022-09-06 21:26:29 INFO  BlockManager:54 - external shuffle service port = 7337\n",
      "2022-09-06 21:26:29 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 100.117.121.240, 42261, None)\n",
      "2022-09-06 21:26:29 INFO  JettyUtils:54 - Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\n",
      "2022-09-06 21:26:29 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@23c7cb18{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2022-09-06 21:26:29 INFO  EventLoggingListener:54 - Logging events to hdfs:/var/log/spark/apps/application_1654538887658_324476\n",
      "2022-09-06 21:26:29 WARN  Utils:66 - spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n",
      "2022-09-06 21:26:29 INFO  Utils:54 - Using initial executors = 2, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "2022-09-06 21:26:29 INFO  ApplicationProperties:98 - Looking for atlas-application.properties in classpath\n",
      "2022-09-06 21:26:29 INFO  ApplicationProperties:111 - Loading atlas-application.properties from file:/mnt/dwi_client/emr-client-config-spark-all/default/<emr cluster>/0ccf384a53c54469a8c91902e4234ac5/emr-client-config-spark-all_default_uncompressed/silver/spark/2.3.4.1/atlas-application.properties\n",
      "2022-09-06 21:26:29 INFO  ApplicationProperties:262 - Property (set to default) atlas.graph.cache.db-cache = true\n",
      "2022-09-06 21:26:29 INFO  ApplicationProperties:262 - Property (set to default) atlas.graph.cache.db-cache-clean-wait = 20\n",
      "2022-09-06 21:26:29 INFO  ApplicationProperties:262 - Property (set to default) atlas.graph.cache.db-cache-size = 0.5\n",
      "2022-09-06 21:26:29 INFO  ApplicationProperties:262 - Property (set to default) atlas.graph.cache.tx-cache-size = 15000\n",
      "2022-09-06 21:26:29 INFO  ApplicationProperties:262 - Property (set to default) atlas.graph.cache.tx-dirty-size = 120\n",
      "2022-09-06 21:26:29 INFO  KafkaNotification:91 - ==> KafkaNotification()\n",
      "2022-09-06 21:26:29 INFO  KafkaNotification:117 - <== KafkaNotification()\n",
      "2022-09-06 21:26:29 INFO  AtlasHook:142 - Created Atlas Hook\n",
      "2022-09-06 21:26:29 INFO  SparkContext:54 - Registered listener com.hortonworks.spark.atlas.SparkAtlasEventTracker\n",
      "2022-09-06 21:26:29 INFO  SparkContext:54 - Registered listener com.airbnb.di.spark.listeners.loader.CombinedListener\n",
      "2022-09-06 21:26:29 INFO  YarnClientSchedulerBackend:54 - SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)\n",
      "Join range to fill PartitionRange(2022-01-14,2022-03-14)\n",
      "IcebergCheck: Checked table <tablename> is not iceberg format.\n",
      "\n",
      "Earliest hole at 2022-01-14 in output table tmp.chronon_test_USER_polynote_join_test_v1, relative to <tablename>\n",
      "Input Parts   : Array(2022-01-14, 2022-01-15, 2022-01-16, 2022-01-17, 2022-01-18, 2022-01-19, 2022-01-20, 2022-01-21, 2022-01-22, 2022-01-23, 2022-01-24, 2022-01-25, 2022-01-26, 2022-01-27, 2022-01-28, 2022-01-29, 2022-01-30, 2022-01-31, 2022-02-01, 2022-02-02, 2022-02-03, 2022-02-04, 2022-02-05, 2022-02-06, 2022-02-07, 2022-02-08, 2022-02-09, 2022-02-10, 2022-02-11, 2022-02-12, 2022-02-13, 2022-02-14, 2022-02-15, 2022-02-16, 2022-02-17, 2022-02-18, 2022-02-19, 2022-02-20, 2022-02-21, 2022-02-22, 2022-02-23, 2022-02-24, 2022-02-25, 2022-02-26, 2022-02-27, 2022-02-28, 2022-03-01, 2022-03-02, 2022-03-03, 2022-03-04, 2022-03-05, 2022-03-06, 2022-03-07, 2022-03-08, 2022-03-09, 2022-03-10, 2022-03-11, 2022-03-12, 2022-03-13, 2022-03-14)\n",
      "Output Parts  : Array()\n",
      "Dropping Parts: Array() \n",
      "          \n",
      "tmp.chronon_test_USER_polynote_join_test_v1 doesn't exist, please double check before drop partitions\n",
      "Building new stats cache for Context(join.offline,chronon_test_USER_polynote_join_test_v1,null,null,false,null,chronon_test,null,null,null)\n",
      "Dropping left unfilled range PartitionRange(2022-01-14,2022-03-14) from join part table tmp.chronon_test_USER_polynote_join_test_v1_chronon_test_test_online_group_by_small_v1\n",
      "IcebergCheck: Checked table <tablename> is not iceberg format.\n",
      "\n",
      "Earliest hole at 2022-01-14 in output table tmp.chronon_test_USER_polynote_join_test_v1_chronon_test_test_online_group_by_small_v1, relative to <tablename>\n",
      "Input Parts   : Array(2022-01-14, 2022-01-15, 2022-01-16, 2022-01-17, 2022-01-18, 2022-01-19, 2022-01-20, 2022-01-21, 2022-01-22, 2022-01-23, 2022-01-24, 2022-01-25, 2022-01-26, 2022-01-27, 2022-01-28, 2022-01-29, 2022-01-30, 2022-01-31, 2022-02-01, 2022-02-02, 2022-02-03, 2022-02-04, 2022-02-05, 2022-02-06, 2022-02-07, 2022-02-08, 2022-02-09, 2022-02-10, 2022-02-11, 2022-02-12, 2022-02-13, 2022-02-14, 2022-02-15, 2022-02-16, 2022-02-17, 2022-02-18, 2022-02-19, 2022-02-20, 2022-02-21, 2022-02-22, 2022-02-23, 2022-02-24, 2022-02-25, 2022-02-26, 2022-02-27, 2022-02-28, 2022-03-01, 2022-03-02, 2022-03-03, 2022-03-04, 2022-03-05, 2022-03-06, 2022-03-07, 2022-03-08, 2022-03-09, 2022-03-10, 2022-03-11, 2022-03-12, 2022-03-13, 2022-03-14)\n",
      "Output Parts  : Array()\n",
      "Dropping Parts: Array() \n",
      "          \n",
      "tmp.chronon_test_USER_polynote_join_test_v1_chronon_test_test_online_group_by_small_v1 doesn't exist, please double check before drop partitions\n",
      "Dropping left unfilled range PartitionRange(2022-01-14,2022-03-14) from join part table tmp.chronon_test_USER_polynote_join_test_v1_chronon_test_test_tmp_v2\n",
      "IcebergCheck: Checked table <tablename> is not iceberg format.\n",
      "IcebergCheck: Checked table tmp.chronon_test_USER_polynote_join_test_v1_chronon_test_test_tmp_v2 is not iceberg format.\n",
      "\n",
      "Earliest hole at 2022-01-14 in output table tmp.chronon_test_USER_polynote_join_test_v1_chronon_test_test_tmp_v2, relative to <tablename>\n",
      "Input Parts   : Array(2022-01-14, 2022-01-15, 2022-01-16, 2022-01-17, 2022-01-18, 2022-01-19, 2022-01-20, 2022-01-21, 2022-01-22, 2022-01-23, 2022-01-24, 2022-01-25, 2022-01-26, 2022-01-27, 2022-01-28, 2022-01-29, 2022-01-30, 2022-01-31, 2022-02-01, 2022-02-02, 2022-02-03, 2022-02-04, 2022-02-05, 2022-02-06, 2022-02-07, 2022-02-08, 2022-02-09, 2022-02-10, 2022-02-11, 2022-02-12, 2022-02-13, 2022-02-14, 2022-02-15, 2022-02-16, 2022-02-17, 2022-02-18, 2022-02-19, 2022-02-20, 2022-02-21, 2022-02-22, 2022-02-23, 2022-02-24, 2022-02-25, 2022-02-26, 2022-02-27, 2022-02-28, 2022-03-01, 2022-03-02, 2022-03-03, 2022-03-04, 2022-03-05, 2022-03-06, 2022-03-07, 2022-03-08, 2022-03-09, 2022-03-10, 2022-03-11, 2022-03-12, 2022-03-13, 2022-03-14)\n",
      "Output Parts  : Array()\n",
      "Dropping Parts: Array() \n",
      "          \n",
      "tmp.chronon_test_USER_polynote_join_test_v1_chronon_test_test_tmp_v2 doesn't exist, please double check before drop partitions\n",
      "Join ranges to compute: \n",
      "    PartitionRange(2022-01-14,2022-02-12),\n",
      "    PartitionRange(2022-02-13,2022-03-14)\n",
      "\n",
      "Computing join for range: PartitionRange(2022-01-14,2022-02-12)  | [1/2]\n",
      "\n",
      "----[Running query]----\n",
      "SELECT\n",
      "  m_dated_<metric> as `m_dated_<metric>`,\n",
      "  UNIX_TIMESTAMP(ts) * 1000 as `ts`,\n",
      "  m_guests as `m_guests`,\n",
      "  id_product as `listing`,\n",
      "  `ds`\n",
      "FROM <tablename> \n",
      "WHERE\n",
      "  ds >= '2022-01-14' AND ds <= '2022-02-12' AND hash(id_product) % 256 = 0 AND <dimension> = 'VALUE'\n",
      "----[End of Query]----\n",
      "\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      " [STARTED] Generating bloom filter on key `listing` for range PartitionRange(2022-01-14,2022-02-12) from <tablename>\n",
      " Approximate distinct count of `listing`: 14\n",
      " Total count of rows: 6676\n",
      "\n",
      "\n",
      " [FINISHED] Generating bloom filter on key `listing` for range PartitionRange(2022-01-14,2022-02-12) from <tablename>\n",
      " Approximate distinct count of `listing`: 14\n",
      " Total count of rows: 6676\n",
      " BloomSize in bits: 128\n",
      "\n",
      "IcebergCheck: Checked table tmp.chronon_test_USER_polynote_join_test_v1_chronon_test_test_tmp_v2 is not iceberg format.\n",
      "IcebergCheck: Checked table <tablename> is not iceberg format.\n",
      "IcebergCheck: Checked table <tablename> is not iceberg format.\n",
      "\n",
      "Unfilled range computation:                             \n",
      "   Output table: tmp.chronon_test_USER_polynote_join_test_v1_chronon_test_test_online_group_by_small_v1\n",
      "   Missing output partitions: Set(2022-01-20, 2022-01-25, 2022-01-14, 2022-01-19, 2022-01-30, 2022-01-15, 2022-02-01, 2022-02-08, 2022-02-12, 2022-02-06, 2022-01-29, 2022-02-09, 2022-02-04, 2022-02-11, 2022-01-24, 2022-01-16, 2022-01-23, 2022-02-10, 2022-02-03, 2022-01-26, 2022-01-27, 2022-01-22, 2022-01-17, 2022-01-21, 2022-01-31, 2022-02-02, 2022-01-18, 2022-02-05, 2022-02-07, 2022-01-28)\n",
      "   Missing input partitions: List()\n",
      "   Unfilled Partitions: Set(2022-01-20, 2022-01-25, 2022-01-14, 2022-01-19, 2022-01-30, 2022-01-15, 2022-02-01, 2022-02-08, 2022-02-12, 2022-02-06, 2022-01-29, 2022-02-09, 2022-02-04, 2022-02-11, 2022-01-24, 2022-01-16, 2022-01-23, 2022-02-10, 2022-02-03, 2022-01-26, 2022-01-27, 2022-01-22, 2022-01-17, 2022-01-21, 2022-01-31, 2022-02-02, 2022-01-18, 2022-02-05, 2022-02-07, 2022-01-28)\n",
      "\n",
      "Right unfilled range for tmp.chronon_test_USER_polynote_join_test_v1_chronon_test_test_online_group_by_small_v1 is Some(PartitionRange(2022-01-14,2022-02-12)) with leftRange of PartitionRange(2022-01-14,2022-02-12)\n",
      "Writing to join part table: tmp.chronon_test_USER_polynote_join_test_v1_chronon_test_test_online_group_by_small_v1\n",
      "\n",
      "JoinPart Info:\n",
      "  part name : chronon_test.test_online_group_by_small.v1, \n",
      "  left type : Events, \n",
      "  right type: Events, \n",
      "  accuracy  : SNAPSHOT,\n",
      "  part unfilled range: PartitionRange(2022-01-14,2022-02-12),\n",
      "  bloom sizes: \n",
      "    listing -> 128\n",
      "\n",
      "  groupBy: GroupBy(metaData:MetaData(name:chronon_test.test_online_group_by_small.v1, online:true, production:true, customJson:{\"team_override\": \"ml_infra\", \"lag\": 0}, dependencies:[{\"name\": \"wait_for_global.pdp_fct_views_global_pdp_fct_views_ds_ds\", \"spec\": \"<tablename>/ds={{ ds }}\", \"start\": \"2022-01-14\", \"end\": null}], tableProperties:{abb_retention_config_json={\"policy\": \"delete_by_last_modified\", \"days\": 30}}, outputNamespace:chronon_test, team:chronon_test), sources:[<Source events:EventSource(table:<tablename>, query:Query(selects:{listing=id_product, m_guests=m_guests, m_dated_<metric>=m_dated_<metric>}, wheres:[hash(id_product) % 256 = 0, <dimension> = 'VALUE'], startPartition:2022-01-14, timeColumn:UNIX_TIMESTAMP(ts) * 1000, setups:[]), isCumulative:false)>], keyColumns:[listing], aggregations:[Aggregation(inputColumn:m_guests, operation:SUM, argMap:{}, windows:[Window(length:7, timeUnit:DAYS)]), Aggregation(inputColumn:m_dated_<metric>, operation:SUM, argMap:{})], accuracy:SNAPSHOT, backfillStartDate:2022-01-14)\n",
      "\n",
      "\n",
      "Unfilled range computation:                             \n",
      "   Output table: tmp.chronon_test_USER_polynote_join_test_v1_chronon_test_test_tmp_v2\n",
      "   Missing output partitions: Set(2022-01-20, 2022-01-25, 2022-01-14, 2022-01-19, 2022-01-30, 2022-01-15, 2022-02-01, 2022-02-08, 2022-02-12, 2022-02-06, 2022-01-29, 2022-02-09, 2022-02-04, 2022-02-11, 2022-01-24, 2022-01-16, 2022-01-23, 2022-02-10, 2022-02-03, 2022-01-26, 2022-01-27, 2022-01-22, 2022-01-17, 2022-01-21, 2022-01-31, 2022-02-02, 2022-01-18, 2022-02-05, 2022-02-07, 2022-01-28)\n",
      "   Missing input partitions: List()\n",
      "   Unfilled Partitions: Set(2022-01-20, 2022-01-25, 2022-01-14, 2022-01-19, 2022-01-30, 2022-01-15, 2022-02-01, 2022-02-08, 2022-02-12, 2022-02-06, 2022-01-29, 2022-02-09, 2022-02-04, 2022-02-11, 2022-01-24, 2022-01-16, 2022-01-23, 2022-02-10, 2022-02-03, 2022-01-26, 2022-01-27, 2022-01-22, 2022-01-17, 2022-01-21, 2022-01-31, 2022-02-02, 2022-01-18, 2022-02-05, 2022-02-07, 2022-01-28)\n",
      "\n",
      "Right unfilled range for tmp.chronon_test_USER_polynote_join_test_v1_chronon_test_test_tmp_v2 is Some(PartitionRange(2022-01-14,2022-02-12)) with leftRange of PartitionRange(2022-01-14,2022-02-12)\n",
      "Writing to join part table: tmp.chronon_test_USER_polynote_join_test_v1_chronon_test_test_tmp_v2\n",
      "\n",
      "JoinPart Info:\n",
      "  part name : chronon_test.test_tmp.v2, \n",
      "  left type : Events, \n",
      "  right type: Events, \n",
      "  accuracy  : SNAPSHOT,\n",
      "  part unfilled range: PartitionRange(2022-01-14,2022-02-12),\n",
      "  bloom sizes: \n",
      "    listing -> 128\n",
      "\n",
      "  groupBy: GroupBy(metaData:MetaData(name:chronon_test.test_tmp.v2, online:true, production:true, customJson:{\"team_override\": \"ml_infra\", \"lag\": 0}, dependencies:[{\"name\": \"wait_for_global.pdp_fct_views_global_pdp_fct_views_ds_ds\", \"spec\": \"<tablename>/ds={{ ds }}\", \"start\": \"2022-01-14\", \"end\": null}], tableProperties:{abb_retention_config_json={\"policy\": \"delete_by_last_modified\", \"days\": 30}}, outputNamespace:chronon_test, team:chronon_test), sources:[<Source events:EventSource(table:<tablename>, query:Query(selects:{listing=id_product, m_guests=m_guests, m_dated_<metric>=m_dated_<metric>}, wheres:[hash(id_product) % 256 = 0, <dimension> = 'VALUE'], startPartition:2022-01-14, timeColumn:UNIX_TIMESTAMP(ts) * 1000, setups:[]), isCumulative:false)>], keyColumns:[listing], aggregations:[Aggregation(inputColumn:m_guests, operation:SUM, argMap:{}, windows:[Window(length:7, timeUnit:DAYS)]), Aggregation(inputColumn:m_dated_<metric>, operation:SUM, argMap:{})], accuracy:SNAPSHOT, backfillStartDate:2022-01-14)\n",
      "\n",
      "Computed Range for ts - min: 1642118519000, max: 1644710388000\n",
      "left unfilled time range: TimeRange(1642118519000,1644710388000)\n",
      "\n",
      "----[Processing GroupBy: chronon_test.test_online_group_by_small.v1]----\n",
      "\n",
      "Rendering source query:\n",
      "   query range: PartitionRange(2022-01-13,2022-02-11)\n",
      "   query window: None\n",
      "   source table: <tablename>\n",
      "   source data range: PartitionRange(2022-01-14,2022-02-11)\n",
      "   source start/end: 2022-01-14/null\n",
      "   source data model: Events\n",
      "   queryable data range: PartitionRange(null,2022-02-11)\n",
      "   intersected/effective scan range: PartitionRange(2022-01-14,2022-02-11)\n",
      "   metaColumns: Map(ds -> null, ts -> UNIX_TIMESTAMP(ts) * 1000)\n",
      "\n",
      "\n",
      "----[Running query]----\n",
      "SELECT\n",
      "  m_dated_<metric> as `m_dated_<metric>`,\n",
      "  UNIX_TIMESTAMP(ts) * 1000 as `ts`,\n",
      "  m_guests as `m_guests`,\n",
      "  id_product as `listing`,\n",
      "  `ds`\n",
      "FROM <tablename> \n",
      "WHERE\n",
      "  hash(id_product) % 256 = 0 AND <dimension> = 'VALUE' AND ds >= '2022-01-14' AND ds <= '2022-02-11'\n",
      "----[End of Query]----\n",
      "\n",
      "Left bounds: 1d->unbounded \n",
      "minQueryTs = 2022-01-13 00:00:00\n",
      "Generating key builder over keys:\n",
      "  string : listing\n",
      "\n",
      "\n",
      "----[Running query]----\n",
      "CREATE TABLE tmp.chronon_test_USER_polynote_join_test_v1_chronon_test_test_online_group_by_small_v1 (\n",
      "    listing string,\n",
      "    m_guests_sum_7d bigint,\n",
      "    m_dated_<metric>_sum bigint\n",
      ")\n",
      "PARTITIONED BY (\n",
      "    ds string\n",
      ")\n",
      "STORED AS PARQUET\n",
      "TBLPROPERTIES (\n",
      "    'semantic_hash'='{\"tmp.chronon_test_USER_polynote_join_test_v1_chronon_test_test_online_group_by_small_v1\":\"IT8GDfjj+A\",\"tmp.chronon_test_USER_polynote_join_test_v1_chronon_test_test_tmp_v2\":\"IT8GDfjj+A\",\"left_source\":\"3Vd0rx0x08\"}'\n",
      ")\n",
      "----[End of Query]----\n",
      "\n",
      "/srv/dwi_client/spark/launcher_scripts/emr-spark-submit: line 153:  7912 Killed                  env SPARK_HOME=$spark_home HADOOP_CONF_DIR=$hadoop_conf_dir SPARK_CONF_DIR=$hadoop_conf_dir \"$spark_bin_path\" $context_args $conf_overrides \"$@\"\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['bash', '/home/USER/notebooks_home/repo/chronon/scripts/spark_submit.sh', '--class', 'ai.chronon.spark.Driver', '/tmp/spark_uber_2.11-0.0.7-assembly.jar', 'join', '--conf-path=/home/USER/notebooks_home/repo/chronon/production/joins/chronon_test/USER_polynote_join_test.v1', '--end-date=2022-01-28']' returned non-zero exit status 137.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4108/2012683394.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun_test_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Time taken: {(datetime.now() - start_time).total_seconds()} seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4108/3857963320.py\u001b[0m in \u001b[0;36mrun_test_config\u001b[0;34m(obj, days)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mjar_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_jar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"USER\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdays\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjar_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/default_user/.conda/envs/user/lib/python3.7/site-packages/ai/chronon/repo/run.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             )\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/default_user/.conda/envs/user/lib/python3.7/site-packages/ai/chronon/repo/run.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(cmd)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running command: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/default_user/.conda/envs/user/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcmd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopenargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['bash', '/home/USER/notebooks_home/repo/chronon/scripts/spark_submit.sh', '--class', 'ai.chronon.spark.Driver', '/tmp/spark_uber_2.11-0.0.7-assembly.jar', 'join', '--conf-path=/home/USER/notebooks_home/repo/chronon/production/joins/chronon_test/USER_polynote_join_test.v1', '--end-date=2022-01-28']' returned non-zero exit status 137."
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "run_test_config(v1, 14)\n",
    "print(f\"Time taken: {(datetime.now() - start_time).total_seconds()} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2e19b1-f169-46a9-ad56-8d446deba74f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (User)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
