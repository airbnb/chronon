{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "text"
   },
   "source": [
    "# Chronon Join Tester\n",
    "\n",
    "\n",
    "This notebook allows you to create and compute a Join for a small set of data using sampled keys.\n",
    "\n",
    "\n",
    "The output table will be written to the tmp namespace, so it won't interfere with production. You can use this to create a new join, test evolving an existing join and comparing with production.\n",
    "\n",
    "\n",
    "How it works?\n",
    "Cell#2 Contains scala functions to build a join that's similar to the original join, but with added filters on keys. The filters are deterministic (hash(key) % number = 0), the higher the modulo the bigger the filter. MetaData is highjacked to set outputNamespace to \"tmp\".\n",
    "\n",
    "\n",
    "This notebook is cells are such as:\n",
    "\n",
    "\n",
    "1. \n",
    "   Python setup (feel free to modify the conf name)\n",
    "   \n",
    "   \n",
    "   \n",
    "2. \n",
    "   Scala library (do not change)\n",
    "   \n",
    "   \n",
    "   \n",
    "3. \n",
    "   Python join config (modify as will)\n",
    "   \n",
    "   \n",
    "   \n",
    "4. \n",
    "   Runner (run the config for 3 or any other config in your branch)\n",
    "   \n",
    "   \n",
    "   \n",
    "\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell.metadata.exec_info": {
     "endTs": 1662069058523,
     "startTs": 1662069057483
    },
    "language": "python"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define some global variables for the sampling run.\n",
    "\"\"\"\n",
    "from ai.chronon.repo.validator import ChrononRepoValidator\n",
    "from ai.chronon.repo.compile import _write_obj\n",
    "from mock import patch\n",
    "\n",
    "import getpass\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Global vars.\n",
    "user = getpass.getuser()\n",
    "home = f\"/home/{user}/notebooks_home\"\n",
    "chronon_root = f\"{home}/repo/chronon\"\n",
    "output_folder = \"production\"\n",
    "output_root = os.path.join(chronon_root, output_folder)\n",
    "\n",
    "# Config specific vars\n",
    "team = \"chronon_test\"\n",
    "conf_name = f\"{user}_polynote_join_test.v0\"\n",
    "metadata_name = \".\".join([team, conf_name])\n",
    "conf = os.path.join(output_root, \"joins\", team, conf_name)\n",
    "\n",
    "# Add chronon modules to the path.\n",
    "sys.path.append(chronon_root)\n",
    "\n",
    "# Utils for notebook.\n",
    "class Stub:\n",
    "    def __init__(self, team):\n",
    "        self.filename = f\"{team}/stub\"\n",
    "\n",
    "def stub(filename):\n",
    "    return [None, Stub(filename)]\n",
    "\n",
    "def write_tmp_config(obj):\n",
    "    obj.metaData.name = metadata_name\n",
    "    obj.metaData.team = team\n",
    "    return _write_obj(\n",
    "        output_root, \n",
    "        validator=ChrononRepoValidator(\n",
    "            chronon_root_path=chronon_root, \n",
    "            output_root=output_folder), \n",
    "        name=metadata_name, \n",
    "        obj=obj, \n",
    "        log_level=None,\n",
    "        force_compile=False,\n",
    "        force_overwrite=True\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell.metadata.exec_info": {
     "startTs": 1662069063493
    },
    "jupyter.outputs_hidden": true,
    "language": "scala"
   },
   "outputs": [],
   "source": [
    "/**\n",
    " * Receive python set up and run a sampled join between start_partition and start_partition + days\n",
    " */\n",
    "import ai.chronon.spark.Driver.parseConf\n",
    "import ai.chronon.api.{Join, Source, QueryUtils, Constants, Builders, JoinPart, GroupBy, MetaData}\n",
    "import ai.chronon.api.Extensions._\n",
    "import ai.chronon.spark.Extensions._\n",
    "import ai.chronon.spark.TableUtils\n",
    "import ai.chronon.spark.{Join => JoinRunner}\n",
    "\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import scala.collection.JavaConverters._\n",
    "\n",
    "val tableUtils = TableUtils(spark)\n",
    "\n",
    "// Sample a source keys based on a modulo value\n",
    "def sampledSource(source: Source, keyFilter: Map[String, Int]): Source = {\n",
    "    val ogQuery = source.query\n",
    "    val newQuery = Builders.Query(\n",
    "        selects = ogQuery.selects.asScala.toMap,\n",
    "        startPartition = ogQuery.startPartition,\n",
    "        endPartition = ogQuery.endPartition,\n",
    "        setups = ogQuery.setups.asScala,\n",
    "        wheres = Option(ogQuery.wheres).map(_.asScala).getOrElse(Seq.empty[String]) ++ ogQuery.selects.asScala.filter{\n",
    "            case (key, value) => keyFilter.contains(key)\n",
    "            }.map{ \n",
    "                case (key, value) => s\"HASH($value) % ${keyFilter(key)} = 0\" }.toSeq,\n",
    "        timeColumn = ogQuery.timeColumn)\n",
    "    if (source.isSetEntities) {\n",
    "        Builders.Source.entities(query = newQuery, snapshotTable = source.table)\n",
    "    } else {\n",
    "        Builders.Source.events(query = newQuery, table = source.table)\n",
    "    }\n",
    "}\n",
    "\n",
    "def joinKeys(join: Join): Seq[String] = join.joinParts.asScala.flatMap{ jp: JoinPart => jp.rightToLeft.keys }.distinct\n",
    "\n",
    "def simpleFilter(join: Join, value: Int): Map[String, Int] = joinKeys(join).map(k => k -> value).toMap\n",
    "\n",
    "def filteredGroupBy(groupBy: GroupBy, filterMap: Map[String, Int]): GroupBy = Builders.GroupBy(\n",
    "    metaData = metadataToTmp(groupBy.metaData), \n",
    "    sources = groupBy.sources.asScala.map(sampledSource(_, filterMap)),\n",
    "    keyColumns = groupBy.keyColumns.asScala,\n",
    "    aggregations = groupBy.aggregations.asScala,\n",
    "    accuracy = groupBy.accuracy)\n",
    "\n",
    "def filteredJoin(join: Join, filterValue: Int): Join = {\n",
    "    val filter = simpleFilter(join, filterValue)\n",
    "    Builders.Join(\n",
    "        metaData = metadataToTmp(join.metaData),\n",
    "        left = sampledSource(join.left, filter), \n",
    "        joinParts = join.joinParts.asScala.map {\n",
    "            jp => \n",
    "                Builders.JoinPart(\n",
    "                    groupBy = filteredGroupBy(jp.groupBy, filter),\n",
    "                    keyMapping = Option(jp.keyMapping).map(_.asScala.toMap).orNull,\n",
    "                    selectors = jp.selectors.asScala,\n",
    "                    prefix = jp.prefix)\n",
    "            }\n",
    "        )\n",
    "}\n",
    "\n",
    "def metadataToTmp(metaData: MetaData): MetaData = {\n",
    "    metaData.setOutputNamespace(\"tmp\")\n",
    "    metaData\n",
    "}\n",
    "\n",
    "def startDate(join: Join): String = {\n",
    "    Option(join.left.query.startPartition).getOrElse(tableUtils.firstAvailablePartition(join.left.table).get)\n",
    "}\n",
    "\n",
    "def computeSampledJoin(join: Join, modulo: Int = 256, days: Int = 10, steps: Int = 30): Unit = {\n",
    "\n",
    "    // Sample the left side\n",
    "    join.validate()\n",
    "    val testJoin: Join = filteredJoin(join, modulo)\n",
    "    val start = startDate(testJoin)\n",
    "    val end = Constants.Partition.shift(start, days)\n",
    "    val runner = new JoinRunner(testJoin, end, tableUtils)\n",
    "    runner.computeJoin(Some(steps))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell.metadata.exec_info": {
     "endTs": 1655849309569,
     "startTs": 1655849308290
    },
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join(metaData=MetaData(name=None, online=True, production=False, customJson='{\"check_consistency\": false, \"lag\": 0}', dependencies=['{\"name\": \"wait_for_<tablename>_ds\", \"spec\": \"<tablename>/ds={{ ds }}\", \"start\": \"2022-01-14\", \"end\": \"2022-01-14\"}', '{\"name\": \"wait_for_<tablename>_ds\", \"spec\": \"<tablename>/ds={{ ds }}\", \"start\": \"2022-01-14\", \"end\": null}'], tableProperties=None, outputNamespace=None, team=None, modeToEnvMap=None, consistencyCheck=None, samplePercent=None), left=Source(events=EventSource(table='<tablename>', topic=None, query=Query(selects={'listing': 'id_product', 'm_guests': 'm_guests', 'm_dated_<metric>': 'm_dated_<metric>', 'ts': 'UNIX_TIMESTAMP(ts) * 1000'}, wheres=[\"<dimension> = 'VALUE'\"], startPartition='2022-01-14', endPartition='2022-01-14', timeColumn='UNIX_TIMESTAMP(ts) * 1000', setups=[], mutationTimeColumn=None, reversalColumn=None), isCumulative=False), entities=None), joinParts=[JoinPart(groupBy=GroupBy(metaData=MetaData(name='chronon_test.test_online_group_by_small.v1', online=True, production=True, customJson='{\"lag\": 0}', dependencies=['{\"name\": \"wait_for_global.pdp_fct_views_global_pdp_fct_views_ds_ds\", \"spec\": \"<tablename>/ds={{ ds }}\", \"start\": \"2022-01-14\", \"end\": null}'], tableProperties={'abb_retention_config_json': '{\"policy\": \"delete_by_last_modified\", \"days\": 30}'}, outputNamespace='chronon_test', team='chronon_test', modeToEnvMap=None, consistencyCheck=None, samplePercent=None), sources=[Source(events=EventSource(table='<tablename>', topic=None, query=Query(selects={'listing': 'id_product', 'm_guests': 'm_guests', 'm_dated_<metric>': 'm_dated_<metric>'}, wheres=[\"<dimension> = 'VALUE'\"], startPartition='2022-01-14', endPartition=None, timeColumn='UNIX_TIMESTAMP(ts) * 1000', setups=[], mutationTimeColumn=None, reversalColumn=None), isCumulative=False), entities=None)], keyColumns=['listing'], aggregations=[Aggregation(inputColumn='m_guests', operation=7, argMap={}, windows=[Window(length=7, timeUnit=1)], buckets=None), Aggregation(inputColumn='m_dated_<metric>', operation=7, argMap={}, windows=None, buckets=None)], accuracy=None, backfillStartDate='2022-01-14'), keyMapping=None, selectors=None, prefix=None)], skewKeys=None)\n",
      "                Join Team - \u001b[34mchronon_test\u001b[0m\n",
      "                Join Name - \u001b[34mUSER_polynote_join_test.v0\u001b[0m\n",
      "\u001b[33mForce overwrite Join USER_polynote_join_test.v0\u001b[0m\n",
      "          Writing Join to - \u001b[34m/home/USER/notebooks_home/repo/chronon/production/joins/chronon_test/USER_polynote_join_test.v0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "name": "Out",
      "type": "Boolean"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sample Online Join on a small dataset.\n",
    "\"\"\"\n",
    "from airbnb import test_sources\n",
    "from ai.chronon.join import Join, JoinPart\n",
    "from group_bys.chronon_test import test_online_group_by_small\n",
    "from airbnb.data_sources_2 import HiveEventSource\n",
    "from ai.chronon.query import Query, select\n",
    "\n",
    "v1 = Join(\n",
    "    left=HiveEventSource(\n",
    "    namespace='global',\n",
    "    table=\"<tablename>\",\n",
    "    query=Query(\n",
    "        selects=select(\n",
    "            listing=\"id_product\",\n",
    "            m_guests=\"m_guests\",\n",
    "            m_dated_<metric>=\"m_dated_<metric>\"\n",
    "        ),\n",
    "        wheres=[\"<dimension> = 'VALUE'\"],\n",
    "        time_column=\"UNIX_TIMESTAMP(ts) * 1000\",\n",
    "        start_partition=\"2022-01-14\",\n",
    "        end_partition=\"2022-01-14\",\n",
    "    )),\n",
    "    right_parts=[\n",
    "        JoinPart(group_by=test_online_group_by_small.v1),\n",
    "    ],\n",
    "    online=True,\n",
    ")\n",
    "print(v1)\n",
    "write_tmp_config(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell.metadata.exec_info": {
     "endTs": 1655849498117,
     "startTs": 1655849325149
    },
    "language": "scala"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Hashes:\n",
      "New: Map(tmp.chronon_test_USER_polynote_join_test_v0_chronon_test_test_online_group_by_small_v1 -> bogrhLBZnZ, left_source -> FSqyOf2XaM),\n",
      "Old: Map(left_source -> 2Rrw98dJUM, tmp.chronon_test_USER_polynote_join_test_v0_chronon_test_test_online_group_by_small_v1 -> bogrhLBZnZ)\n",
      "Dropping table with command: DROP TABLE IF EXISTS tmp.chronon_test_USER_polynote_join_test_v0_chronon_test_test_online_group_by_small_v1\n",
      "\n",
      "----[Running query]----\n",
      "DROP TABLE IF EXISTS tmp.chronon_test_USER_polynote_join_test_v0_chronon_test_test_online_group_by_small_v1\n",
      "----[End of Query]----\n",
      "\n",
      "Dropping table with command: DROP TABLE IF EXISTS tmp.chronon_test_USER_polynote_join_test_v0\n",
      "\n",
      "----[Running query]----\n",
      "DROP TABLE IF EXISTS tmp.chronon_test_USER_polynote_join_test_v0\n",
      "----[End of Query]----\n",
      "\n",
      "Join range to fill PartitionRange(2022-01-14,2022-01-14)\n",
      "\n",
      "Earliest hole at 2022-01-14 in output table tmp.chronon_test_USER_polynote_join_test_v0, relative to <tablename>\n",
      "Input Parts   : Array(2022-01-14)\n",
      "Output Parts  : Array()\n",
      "Dropping Parts: Array() \n",
      "          \n",
      "tmp.chronon_test_USER_polynote_join_test_v0 doesn't exist, please double check before drop partitions\n",
      "Dropping left unfilled range PartitionRange(2022-01-14,2022-01-14) from join part table tmp.chronon_test_USER_polynote_join_test_v0_chronon_test_test_online_group_by_small_v1\n",
      "\n",
      "Earliest hole at 2022-01-14 in output table tmp.chronon_test_USER_polynote_join_test_v0_chronon_test_test_online_group_by_small_v1, relative to <tablename>\n",
      "Input Parts   : Array(2022-01-14)\n",
      "Output Parts  : Array()\n",
      "Dropping Parts: Array() \n",
      "          \n",
      "tmp.chronon_test_USER_polynote_join_test_v0_chronon_test_test_online_group_by_small_v1 doesn't exist, please double check before drop partitions\n",
      "Join ranges to compute: \n",
      "    PartitionRange(2022-01-14,2022-01-14)\n",
      "\n",
      "Computing join for range: PartitionRange(2022-01-14,2022-01-14)  | [1/1]\n",
      "\n",
      "----[Running query]----\n",
      "SELECT\n",
      "  m_dated_<metric> as `m_dated_<metric>`,\n",
      "  UNIX_TIMESTAMP(ts) * 1000 as `ts`,\n",
      "  m_guests as `m_guests`,\n",
      "  id_product as `listing`,\n",
      "  `ds`\n",
      "FROM <tablename> \n",
      "WHERE\n",
      "  ds >= '2022-01-14' AND ds <= '2022-01-14' AND <dimension> = 'VALUE' AND HASH(id_product) % 256 = 0\n",
      "----[End of Query]----\n",
      "\n",
      "Join keys for chronon_test.test_online_group_by_small.v1: listing, ts, ds\n",
      "Left Schema:\n",
      "  int    : m_dated_<metric>\n",
      "  bigint : ts\n",
      "  bigint : m_guests\n",
      "  string : listing\n",
      "  string : ds\n",
      "  string : ts_ds\n",
      "  \n",
      "Right Schema:\n",
      "  string : listing\n",
      "  bigint : ts\n",
      "  bigint : chronon_test_test_online_group_by_small_v1_m_guests_sum_7d\n",
      "  bigint : chronon_test_test_online_group_by_small_v1_m_dated_<metric>_sum\n",
      "  string : ds\n",
      "  \n",
      "\n",
      "== Physical Plan ==\n",
      "*(5) Project [listing#830, ts#828L, ds#868, m_dated_<metric>#862, m_guests#849L, ts_ds#884, chronon_test_test_online_group_by_small_v1_m_guests_sum_7d#1477L, chronon_test_test_online_group_by_small_v1_m_dated_<metric>_sum#1483L]\n",
      "+- SortMergeJoin [listing#830, ts#828L, ds#868], [listing#1461, ts#1462L, ds#1465], LeftOuter\n",
      "   :- *(2) Sort [listing#830 ASC NULLS FIRST, ts#828L ASC NULLS FIRST, ds#868 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(listing#830, ts#828L, ds#868, 200)\n",
      "   :     +- *(1) Project [m_dated_<metric>#862, (unix_timestamp(ts#865, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)) * 1000) AS ts#828L, m_guests#849L, id_product#832 AS listing#830, ds#868, from_unixtime(cast((cast((unix_timestamp(ts#865, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)) * 1000) as double) / 1000.0) as bigint), yyyy-MM-dd, Some(Etc/UTC)) AS ts_ds#884]\n",
      "   :        +- *(1) Filter ((isnotnull(<dimension>#848) && (<dimension>#848 = VALUE)) && ((hash(id_product#832, 42) % 256) = 0))\n",
      "   :           +- Scan hive <tablename> [<dimension>#848, ds#868, id_product#832, m_dated_<metric>#862, m_guests#849L, ts#865], HiveTableRelation `global`.`<tablename>`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, [id_listing_view_event#831, id_product#832, dim_product_type#833, id_client_session#834, id_impression#835, id_visitor#836, id_user#837, id_search#838, dim_not_bot#839, dim_not_bot_aux#840, dim_pdp_page_type#841, dim_top_level_domain#842, dim_platform#843, dim_mini_app#844, dim_description_language#845, dim_available#846, dim_viewport#847, <dimension>#848, m_guests#849L, m_base_price#850, m_tax#851, m_service_fee#852, m_service_fee_guest#853, m_tax_guest#854, ... 13 more fields], [ds#868], [isnotnull(ds#868), (ds#868 >= 2022-01-14), (ds#868 <= 2022-01-14)]\n",
      "   +- *(4) Sort [listing#1461 ASC NULLS FIRST, ts#1462L ASC NULLS FIRST, ds#1465 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(listing#1461, ts#1462L, ds#1465, 200)\n",
      "         +- *(3) Project [listing#1461, ts#1462L, m_guests_sum_7d#1463L AS chronon_test_test_online_group_by_small_v1_m_guests_sum_7d#1477L, m_dated_<metric>_sum#1464L AS chronon_test_test_online_group_by_small_v1_m_dated_<metric>_sum#1483L, ds#1465]\n",
      "            +- *(3) Filter ((isnotnull(listing#1461) && ((hash(listing#1461, 42) % 256) = 0)) && isnotnull(ts#1462L))\n",
      "               +- Scan hive tmp.chronon_test_USER_polynote_join_test_v0_chronon_test_test_online_group_by_small_v1 [ds#1465, listing#1461, m_dated_<metric>_sum#1464L, m_guests_sum_7d#1463L, ts#1462L], HiveTableRelation `tmp`.`chronon_test_USER_polynote_join_test_v0_chronon_test_test_online_group_by_small_v1`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, [listing#1461, ts#1462L, m_guests_sum_7d#1463L, m_dated_<metric>_sum#1464L], [ds#1465], [isnotnull(ds#1465), (ds#1465 >= 2022-01-14), (ds#1465 <= 2022-01-14)]\n",
      "\n",
      "----[Running query]----\n",
      "CREATE TABLE tmp.chronon_test_USER_polynote_join_test_v0 (\n",
      "    listing string,\n",
      "    ts bigint,\n",
      "    m_dated_<metric> int,\n",
      "    m_guests bigint,\n",
      "    chronon_test_test_online_group_by_small_v1_m_guests_sum_7d bigint,\n",
      "    chronon_test_test_online_group_by_small_v1_m_dated_<metric>_sum bigint\n",
      ")\n",
      "PARTITIONED BY (\n",
      "    ds string\n",
      ")\n",
      "STORED AS PARQUET\n",
      "TBLPROPERTIES (\n",
      "    'semantic_hash'='{\"tmp.chronon_test_USER_polynote_join_test_v0_chronon_test_test_online_group_by_small_v1\":\"bogrhLBZnZ\",\"left_source\":\"FSqyOf2XaM\"}'\n",
      ")\n",
      "----[End of Query]----\n",
      "\n",
      "206 rows requested to be written into table tmp.chronon_test_USER_polynote_join_test_v0\n",
      "repartitioning data for table tmp.chronon_test_USER_polynote_join_test_v0 into 1 rdd partitions\n",
      "Finished writing to tmp.chronon_test_USER_polynote_join_test_v0\n",
      "Wrote to table tmp.chronon_test_USER_polynote_join_test_v0, into partitions: PartitionRange(2022-01-14,2022-01-14) | [1/1] in 2 mins\n",
      "Wrote to table tmp.chronon_test_USER_polynote_join_test_v0, into partitions: PartitionRange(2022-01-14,2022-01-14)\n",
      "\n",
      "----[Running query]----\n",
      "SELECT\n",
      "  *\n",
      "FROM tmp.chronon_test_USER_polynote_join_test_v0 \n",
      "WHERE\n",
      "  ds >= '2022-01-14' AND ds <= '2022-01-14'\n",
      "----[End of Query]----\n",
      "\n",
      "+--------+-------------+-----------------+--------+----------------------------------------------------------+----------------------------------------------------------------+----------+\n",
      "| listing|           ts|m_dated_<metric>|m_guests|chronon_test_test_online_group_by_small_v1_m_guests_sum_7d|chronon_test_test_online_group_by_small_v1_m_dated_<metric>_sum|        ds|\n",
      "+--------+-------------+-----------------+--------+----------------------------------------------------------+----------------------------------------------------------------+----------+\n",
      "|32491886|1642190625000|                1|       4|                                                        81|                                                              42|2022-01-14|\n",
      "|49126113|1642140301000|                0|       1|                                                        22|                                                               2|2022-01-14|\n",
      "|20473214|1642136105000|                0|       1|                                                        30|                                                               7|2022-01-14|\n",
      "|32491886|1642204569000|                1|       1|                                                       100|                                                              55|2022-01-14|\n",
      "|32491886|1642164650000|                1|       1|                                                        29|                                                              19|2022-01-14|\n",
      "|24231546|1642177512000|                1|       1|                                                         7|                                                               2|2022-01-14|\n",
      "|32491886|1642160668000|                0|       0|                                                        24|                                                              14|2022-01-14|\n",
      "|40087821|1642120844000|                0|       1|                                                         2|                                                               1|2022-01-14|\n",
      "|49126113|1642128987000|                0|       1|                                                        12|                                                               2|2022-01-14|\n",
      "|20473214|1642187998000|                1|       4|                                                       100|                                                              16|2022-01-14|\n",
      "|20204327|1642175200000|                1|       8|                                                         7|                                                               1|2022-01-14|\n",
      "|20473214|1642121224000|                1|       4|                                                      null|                                                            null|2022-01-14|\n",
      "|40087821|1642120879000|                1|       1|                                                         3|                                                               1|2022-01-14|\n",
      "|20473214|1642150235000|                0|       1|                                                        39|                                                               7|2022-01-14|\n",
      "|20470232|1642176334000|                0|       1|                                                         3|                                                               0|2022-01-14|\n",
      "|32491886|1642143303000|                1|       1|                                                         3|                                                               1|2022-01-14|\n",
      "|32491886|1642157847000|                1|       1|                                                        14|                                                               4|2022-01-14|\n",
      "|32491886|1642203306000|                1|       1|                                                        99|                                                              54|2022-01-14|\n",
      "|32491886|1642176117000|                1|       1|                                                        67|                                                              28|2022-01-14|\n",
      "|20473214|1642155545000|                0|       1|                                                        41|                                                               7|2022-01-14|\n",
      "+--------+-------------+-----------------+--------+----------------------------------------------------------+----------------------------------------------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Example: Run a sampled join for a specific conf.\n",
    "val join: Join = parseConf[Join](conf)\n",
    "computeSampledJoin(join, modulo = 256, days = 5, steps = 10)\n",
    "val outputDf = spark.sql(s\"SELECT * FROM ${join.metaData.outputTable}\")\n",
    "outputDf.show()\n",
    "outputDf[\"\"]"
   ]
  }
 ],
 "metadata": {
  "config": {
   "dependencies": {
    "scala": [
     "ai.chronon:spark_embedded_2.11:0.0.7"
    ]
   },
   "env": {},
   "exclusions": [],
   "repositories": [],
   "sparkConfig": {
    "hive.exec.dynamic.partition.mode": "nonstrict",
    "spark.master": "yarn"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (User)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
